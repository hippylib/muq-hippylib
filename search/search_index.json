{
    "docs": [
        {
            "location": "/",
            "text": "MUQ - hIPPYlib Integration\n\n\nRecent years have seen a massive explosion of datasets across all areas of science,\nengineering, technology, medicine, and the social sciences.\nThe central questions are: How do we optimally learn from data through the lens of models?\nAnd how do we do so taking into account uncertainty in both data and models?\n\n\nThese questions can be mathematically framed as Bayesian inverse problems.\nWhile powerful and sophisticated approaches have been developed to tackle these problems,\nsuch methods are often challenging to implement and typically require first\nand second order derivatives that are not always available in existing computational models.\n\n\nThe MUQ-hIPPYlib integrated framework overcomes this hurdle by providing unprecedented access\nto state-of-the-art algorithms for deterministic and Bayesian inverse problems.\nMUQ provides a spectrum of powerful Bayesian inversion models and algorithms,\nbut expects forward models to come equipped with gradients/Hessians to permit large-scale solution.\nhIPPYlib implements powerful large-scale gradient/Hessian-based solvers in an environment\nthat can automatically generate needed derivatives, but it lacks full Bayesian capabilities.\nBy integrating these two libraries, we created a robust, scalable, and efficient software\nframework that realizes the benefits of each to tackle complex large-scale Bayesian inverse\nproblems across a broad spectrum of scientific and engineering areas.\n\n\nHow to use this framework\n\n\nFor an overview of the framework please checkout our poster presentations:\n- \n2017 CSSI PI Meeting\n\n- \n2020 CSSI PI Meeting\n\n\nThe \nSingle-phase subsurface inference problem\n is best place to \nlearn about the framework.\n\n\nFor additional resources and tutorials please see:\n\n\n\n\nThe teaching material for the \n2018 Gene Golub SIAM Summer School\n on \nInverse Problems: Systematic Integration\nof Data with Models under Uncertainty\n available \nhere\n.\n\n\nThe numerous tutorial examples on the \nMUQ\n\nand \nhIPPYlib\n websites.\n\n\n\n\nCode availability and downloads\n\n\n\n\nMUQ: \nmuq.mit.edu\n; \nbitbucket\n\n\nhIPPYlib: \nhippylib.github.io\n; \ngithub\n\n\n\n\nDocker images and conda packages\n\n\nComing soon!\n\n\nCitations\n\n\nWe are working on a manuscript describing the integrated MUQ-hIPPYlib framework.\nFor the time being please cite\n\n\nOmar Ghattas, Ki-Tae Kim, Youssef Marzouk, Matthew Parno, Noemi Petra, and Umberto Villa. \nSI2-SSI: Integrating Data with Complex Predictive Models under Uncertainty: \nAn Extensible Software Framework for Large-Scale Bayesian Inversion.\nFigshare, 2020, https://doi.org/10.6084/m9.figshare.11803518",
            "title": "Home"
        },
        {
            "location": "/#muq-hippylib-integration",
            "text": "Recent years have seen a massive explosion of datasets across all areas of science,\nengineering, technology, medicine, and the social sciences.\nThe central questions are: How do we optimally learn from data through the lens of models?\nAnd how do we do so taking into account uncertainty in both data and models?  These questions can be mathematically framed as Bayesian inverse problems.\nWhile powerful and sophisticated approaches have been developed to tackle these problems,\nsuch methods are often challenging to implement and typically require first\nand second order derivatives that are not always available in existing computational models.  The MUQ-hIPPYlib integrated framework overcomes this hurdle by providing unprecedented access\nto state-of-the-art algorithms for deterministic and Bayesian inverse problems.\nMUQ provides a spectrum of powerful Bayesian inversion models and algorithms,\nbut expects forward models to come equipped with gradients/Hessians to permit large-scale solution.\nhIPPYlib implements powerful large-scale gradient/Hessian-based solvers in an environment\nthat can automatically generate needed derivatives, but it lacks full Bayesian capabilities.\nBy integrating these two libraries, we created a robust, scalable, and efficient software\nframework that realizes the benefits of each to tackle complex large-scale Bayesian inverse\nproblems across a broad spectrum of scientific and engineering areas.",
            "title": "MUQ - hIPPYlib Integration"
        },
        {
            "location": "/#how-to-use-this-framework",
            "text": "For an overview of the framework please checkout our poster presentations:\n-  2017 CSSI PI Meeting \n-  2020 CSSI PI Meeting  The  Single-phase subsurface inference problem  is best place to \nlearn about the framework.  For additional resources and tutorials please see:   The teaching material for the  2018 Gene Golub SIAM Summer School  on  Inverse Problems: Systematic Integration\nof Data with Models under Uncertainty  available  here .  The numerous tutorial examples on the  MUQ \nand  hIPPYlib  websites.",
            "title": "How to use this framework"
        },
        {
            "location": "/#code-availability-and-downloads",
            "text": "MUQ:  muq.mit.edu ;  bitbucket  hIPPYlib:  hippylib.github.io ;  github",
            "title": "Code availability and downloads"
        },
        {
            "location": "/#docker-images-and-conda-packages",
            "text": "Coming soon!",
            "title": "Docker images and conda packages"
        },
        {
            "location": "/#citations",
            "text": "We are working on a manuscript describing the integrated MUQ-hIPPYlib framework.\nFor the time being please cite  Omar Ghattas, Ki-Tae Kim, Youssef Marzouk, Matthew Parno, Noemi Petra, and Umberto Villa. \nSI2-SSI: Integrating Data with Complex Predictive Models under Uncertainty: \nAn Extensible Software Framework for Large-Scale Bayesian Inversion.\nFigshare, 2020, https://doi.org/10.6084/m9.figshare.11803518",
            "title": "Citations"
        },
        {
            "location": "/tutorial/",
            "text": "\\def\\data{ {\\bf d}_\\rm{obs}}\n\\def\\vec{\\bf}\n\\def\\m{ {\\bf m}}\n\\def\\map{m_{\\nu}}\n\\def\\postcov{ \\mathcal{C}_{\\nu} }\n\\def\\prcov{ \\mathcal{C}_{\\text{prior}} }\n\\def\\matrix{\\bf}\n\\def\\Hmisfit{ \\mathcal{H}_{\\text{misfit}} }\n\\def\\diag{\\operatorname{diag}}\n\\def\\Vr{{\\matrix V}_r}\n\\def\\Wr{{\\matrix W}_r}\n\\def\\Ir{{\\matrix I}_r}\n\\def\\Dr{{\\matrix D}_r}\n\\def\\H{{\\matrix H} }\n\\def\\matHmis{ {\\H}_{\\rm misfit}}\n\\def\\Gpost{\\boldsymbol{\\Gamma}_{\\nu} }\n\\def\\Gprior{ \\boldsymbol{\\Gamma}_{\\rm prior} }\n\n\n\n\n\nBayesian quantification of parameter uncertainty\n\n\nI. Estimating the posterior pdf of the coefficient parameter field in an elliptic PDE\n\n\nIn this example we tackle the problem of quantifying the\nuncertainty in the solution of an inverse problem governed by an\nelliptic PDE via the Bayesian inference framework. \nHence, we state the inverse problem as a\nproblem of statistical inference over the space of uncertain\nparameters, which are to be inferred from data and a physical\nmodel.  The resulting solution to the statistical inverse problem\nis a posterior distribution that assigns to any candidate set of\nparameter fields, our belief (expressed as a probability) that a\nmember of this candidate set is the \"true\" parameter field that\ngave rise to the observed data.\n\n\nBayes's Theorem\n\n\nThe posterior probability distribution combines the prior pdf\n\n\\mu_{\\text{prior}}(m)\n over the parameter space, which encodes\nany knowledge or assumptions about the parameter space that we may\nwish to impose before the data are considered, with a likelihood pdf\n\n\\pi_{\\text{like}}(\\data \\; | \\; m)\n, which explicitly\nrepresents the probability that a given parameter \nm\n\nmight give rise to the observed data \n\\data \\in\n\\mathbb{R}^{n_t}\n, namely:\n\n\n\n\n\n\\begin{align}\nd \\mu_{\\text{post}}(m | \\data) \\propto \\pi_{\\text{like}}(\\data \\,|\\, m) \\, d\\mu_{\\text{prior}}(m).\n\\end{align}\n\n\n\n\n\nNote that infinite-dimensional analog of Bayes's formula requires the use Radon-Nikodym derivatives instead of probability density functions.\n\n\nThe prior\n\n\nWe consider a Gaussian prior with mean \n{m}_{\\rm prior}\n and covariance \n\\prcov\n, \n\\mu_{\\rm prior} \\sim \\mathcal{N}({m}_{\\rm prior}, \\prcov)\n. The covariance is given by the discretization of the inverse of differential operator \n\\mathcal{A}^{-2} = (-\\gamma \\Delta + \\delta I)^{-2}\n, where \n\\gamma\n, \n\\delta > 0\n control the correlation length and the variance of the prior operator. This choice of prior ensures that it is a trace-class operator, guaranteeing bounded pointwise variance and a well-posed infinite-dimensional Bayesian inverse problem.\n\n\nThe likelihood\n\n\n\n\n\n\\data =  {\\bf f}(m) + {\\bf e }, \\;\\;\\;  {\\bf e} \\sim \\mathcal{N}({\\bf 0}, {\\bf \\Gamma}_{\\text{noise}} )\n\n\n\n\n\n\n\n\n\\pi_{\\text like}(\\data \\; | \\; m)  \\propto \\exp \\left( - \\tfrac{1}{2} \\parallel {\\bf f}(m) - \\data \\parallel^{2}_{{\\bf \\Gamma}_{\\text{noise}}^{-1}}\\right)\n\n\n\n\n\nHere \n{\\bf f}\n is the parameter-to-observable map that takes a parameter \nm\n and maps\nit to the space observation vector \n\\data\n.\n\n\nIn this application, \n{\\bf f}\n consists in the composition of a PDE solve (to compute the state \nu\n) and a pointwise observation of the state \nu\n to extract the observation vector \n\\data\n.\n\n\nThe posterior\n\n\n\n\n\nd\\mu_{\\text{post}}(m \\; | \\; \\data)  \\propto \\exp \\left( - \\tfrac{1}{2} \\parallel {\\bf f}(m) - \\data \\parallel^{2}_{{\\bf \\Gamma}_{\\text{noise}}^{-1}} \\! - \\tfrac{1}{2}\\parallel m - m_{\\rm prior} \\parallel^{2}_{\\prcov^{-1}} \\right)\n\n\n\n\n\nThe Laplace approximation to the posterior: \n\\nu \\sim \\mathcal{N}({\\map},\\bf \\postcov)\n\n\n\n\nThe mean of the Laplace approximation posterior distribution, \n{\\map}\n, is the\nparameter maximizing the posterior, and\nis known as the maximum a posteriori (MAP) point.  It can be found\nby minimizing the negative log of the posterior, which amounts to\nsolving a deterministic inverse problem) with appropriately weighted norms,\n\n\n\n\n\n\\map := \\underset{m}{\\arg \\min} \\; \\mathcal{J}(m) \\;:=\\;\n\\Big( \n\\frac{1}{2} \\| {\\bf f}(m) - \\data \\|^2_{ {\\bf \\Gamma}_{\\text{noise}}^{-1}} \n+\\frac{1}{2} \\| m -m_{\\rm prior} \\|^2_{\\prcov^{-1}} \n\\Big).\n\n\n\n\n\nThe posterior covariance matrix is then given by the inverse of\nthe Hessian matrix of \n\\mathcal{J}\n at \n\\map\n, namely\n\n\n\n\n\n\\postcov = \\left(\\Hmisfit(\\map) + \\prcov^{-1} \\right)^{-1},\n\n\n\n\n\nprovided that \n\\Hmisfit(\\map)\n is positive semidefinite.\n\n\nThe generalized eigenvalue problem\n\n\nIn what follows we denote with \n\\matHmis, \\Gpost, \\Gprior \\in \\mathbb{R}^{n\\times n}\n the matrices stemming from the discretization of the operators \n\\Hmisfit(\\map)\n, \n\\postcov\n, \n\\prcov\n with respect to the unweighted Euclidean inner product.\nThen we considered the symmetric generalized eigenvalue problem\n\n\n\n\n\n \\matHmis {\\matrix V} = \\Gprior^{-1} {\\matrix V} {\\matrix \\Lambda},\n\n\n\n\n\nwhere \n{\\matrix \\Lambda} = \\diag(\\lambda_i) \\in \\mathbb{R}^{n\\times n}\n\ncontains the generalized eigenvalues and the columns of \n{\\matrix V}\\in\n\\mathbb R^{n\\times n}\n the generalized eigenvectors such that \n\n{\\matrix V}^T \\Gprior^{-1} {\\matrix V} = {\\matrix I}\n.\n\n\nRandomized eigensolvers to construct the approximate spectral decomposition\n\n\nWhen the generalized eigenvalues \n\\{\\lambda_i\\}\n decay rapidly, we can\nextract a low-rank approximation of \n\\matHmis\n by retaining only the \nr\n\nlargest eigenvalues and corresponding eigenvectors,\n\n\n\n\n\n \\matHmis \\approx \\Gprior^{-1} \\Vr {\\matrix{\\Lambda}}_r \\Vr^T \\Gprior^{-1}.\n\n\n\n\n\nHere, \n\\Vr \\in \\mathbb{R}^{n\\times r}\n contains only the \nr\n\ngeneralized eigenvectors of \n\\matHmis\n that correspond to the \nr\n largest eigenvalues,\nwhich are assembled into the diagonal matrix \n{\\matrix{\\Lambda}}_r = \\diag\n(\\lambda_i) \\in \\mathbb{R}^{r \\times r}\n.\n\n\nThe approximate posterior covariance\n\n\nUsing the Sherman\u2013Morrison\u2013Woodbury formula, we write\n\n\n\n\n\n\\begin{align}\n  \\notag \\Gpost = \\left(\\matHmis+ \\Gprior^{-1}\\right)^{-1}\n  = \\Gprior^{-1}-\\Vr {\\matrix{D}}_r \\Vr^T +\n  \\mathcal{O}\\left(\\sum_{i=r+1}^{n} \\frac{\\lambda_i}{\\lambda_i +\n    1}\\right),\n\\end{align}\n\n\n\n\n\nwhere \n{\\matrix{D}}_r :=\\diag(\\lambda_i/(\\lambda_i+1)) \\in\n\\mathbb{R}^{r\\times r}\n. The last term in this expression captures the\nerror due to truncation in terms of the discarded eigenvalues; this\nprovides a criterion for truncating the spectrum, namely that \nr\n is\nchosen such that \n\\lambda_r\n is small relative to 1. \n\n\nTherefore we can approximate the posterior covariance as\n\n\n\n\n\n\\Gpost \\approx \\Gprior - \\Vr {\\matrix{D}}_r \\Vr^T\n\n\n\n\n\nDrawing samples from a Gaussian distribution with covariance \n\\Gpost\n\n\n\n\nLet \n{\\bf x}\n be a sample for the prior distribution, i.e. \n{\\bf x} \\sim \\mathcal{N}({\\bf 0}, \\Gprior)\n, then, using the low rank approximation of the posterior covariance, we compute a sample \n{\\bf v} \\sim \\mathcal{N}({\\bf 0}, \\Gpost)\n as\n\n\n\n\n\n  {\\bf v} = \\big\\{ \\Vr \\big[ ({\\matrix{\\Lambda}}_r +\n    \\Ir)^{-1/2} - \\Ir \\big] \\Vr^T\\Gprior^{-1}  + {\\bf I} \\big\\} {\\bf x} \n\n\n\n\n\nFull posterior sampling via Markov chain Monte Carlo (MCMC)\n\n\nThe posterior can be fully explored by using MCMC algorithms, the most popular method for sampling from a probability distribution.\nIn this example, some of the advanced MCMC algorithms are considered and compared in terms of efficiency and accuracy.\n\n\nThe preconditioned Crank-Nicolson algorithm (pCN)\n\n\nThe pCN algorithm is perhaps the simplest MCMC method that is well-defined in the infinite\ndimensional setting ensuring a mixing rates independent of the dimension of the discretized parameter space.\n\n\nThe algorithm proceeds as follows (see \n[Cotter et al. (2013)]\n \n[Pinski et al. (2015)]\n for the details):\n1. Given \nm^{(k)}\n, propose \nv^{(k+1)} = m_{\\rm prop} + \\sqrt{1 - \\beta^2}(m^{(k)} - m_{\\rm prop}) + \\beta \\xi^{(k)}, \\quad \\xi^{(k)} \\sim \\mathcal{N}( 0, \\mathcal{C}_{\\rm prop} )\n\n2. Set \nm^{(k+1)} = v^{(k+1)}\n with probability \na(m^{(k)}, v^{(k+1)}) = \\min \\left(1, \n\\frac{\\mu_{\\text{post}}(v^{(k+1)}) q(v^{(k+1)}, m^{(k)})}{\\mu_{\\text{post}}(m^{(k)}) q(m^{(k)}, v^{(k+1)})} \\right)\n\n\n\n\nwhere \nq(m,v) \\sim \\mathcal{N}\\left( m_{\\rm prop} + \\sqrt{1 - \\beta^2}(m - m_{\\rm prop}), \\beta^2 \\mathcal{C}_{\\rm prop} \\right)\n with proposal mean \nm_{\\rm prop}\n and covariance \n\\mathcal{C}_{\\rm prop}\n and \n\\beta\n is a parameter controlling the step length of the proposal.\n\n\nThe preconditioned Metropolis adjusted Langevin algorithm (MALA)\n\n\nThe MALA algorithm is built on two mechanisms: the overdamped Langevin diffusion to propose a move and the Metropolis\u2013Hastings algorithm to accept or reject the proposal move \n[Roberts and Tweedie (1996)]\n.\n\n\nThe preconditioned MALA algorithm is described as follows:\n1. Given \nm^{(k)}\n, propose\n\nv^{(k+1)} = m^{(k)} + \\tau \\mathcal{A}_{\\rm prop} \\nabla \\log \\mu_{\\text{post}} (m^{(k)}) + \\sqrt{2 \\tau \\mathcal{A}_{\\rm prop}} \\xi^{(k)}, \\quad \\xi^{(k)} \\sim \\mathcal{N}( 0, \\mathcal{I})\n\n2. Set \nm^{(k+1)} = v^{(k+1)}\n with probability \na(m^{(k)}, v^{(k+1)}) = \\min \\left(1, \n\\frac{\\mu_{\\text{post}}(v^{(k+1)}) q(v^{(k+1)}, m^{(k)})}{\\mu_{\\text{post}}(m^{(k)}) q(m^{(k)}, v^{(k+1)})} \\right)\n\n\n\n\nwhere \nq(m,v) \\sim \\mathcal{N}\\left( m + \\tau \\mathcal{A}_{\\rm prop} \\nabla \\log \\mu_{\\text{post}} (m), 2 \\tau \\mathcal{A}_{\\rm prop} \\right)\n with a proposal covariance \n\\mathcal{A}_{\\rm prop}\n and \n\\tau\n is a step size.\n\n\nThe Delayed Rejection (DR)\n\n\nThe basic idea of the delayed rejection is to use a sequence of stages in each iteration.\nUnlike the basic Metropolis-Hastings algorithm, if a candidate is rejected, a new move is proposed.\nThe acceptance rate for the new proposal move is adjusted so that the stationary distribution is preserved.\nFor the details, see \n[Mira (2001)]\n.\n\n\nThis tutorial shows\n\n\n\n\nDefinition of the component of an inverse problem (the forward problem, the prior, and the misfit functional) using hIPPYlib\n\n\nComputation of the maximum a posterior MAP point using inexact Newton-CG algorithm\n\n\nLow-rank based approximation of the posterior covariance under the Laplace Approximation\n\n\nSampling from the prior distribution and Laplace Approximation using hIPPYlib\n\n\nConstruction of a MUQ workgraph using a PDE model defined in hIPPYlib\n\n\nExploring the full posterior using the MCMC methods implemented in MUQ\n\n\nConvergence diagnostics of MCMC simulation results and their comparison\n\n\n\n\nMathematical tools used\n\n\n\n\nFinite element method\n\n\nDerivation of gradient and Hessian via the adjoint method\n\n\nInexact Newton-CG\n\n\nRandomized eigensolvers\n\n\nBayes' formula\n\n\nMCMC methods\n\n\n\n\nList of software used\n\n\nhIPPYlib\n, \nMUQ\n and their interfaces are the main software framework in this tutorial.\nAdditional tools used are:\n\n\n\n\nFEniCS\n, A parallel finite element element library for the discretization of partial differential equations\n\n\nPETSc\n, A set of data structures and routines for scalable and efficient linear algebra operations and solvers\n\n\nNumpy\n, A python package for linear algebra\n\n\nMatplotlib\n, A python package for visualizing the results\n\n\n\n\nReferences\n\n\nCotter, S. L., Roberts, G. O., Stuart, A. M., & White, D. (2013)\n. \nMCMC methods for functions: modifying old algorithms to make them faster. \nStatistical Science, 424-446.\n\n\nPinski, F. J., Simpson, G., Stuart, A. M., & Weber, H. (2015)\n. \nAlgorithms for Kullback--Leibler approximation of probability measures in infinite dimensions. \nSIAM Journal on Scientific Computing, 37(6), A2733-A2757.\n\n\nRoberts, G. O., & Tweedie, R. L. (1996)\n. \nExponential convergence of Langevin distributions and their discrete approximations. \nBernoulli, 2(4), 341-363.\n\n\nMira, A. (2001)\n.\nOn Metropolis-Hastings algorithms with delayed rejection.\nMetron, 59(3-4), 231-241.\n\n\nII. hIPPYlib-MUQ integration\n\n\nThe main objective of this example is to illustrate the interface between \nhIPPYlib\n and \nMUQ\n.\n\n\nWe make use of \nhIPPYlib\n to\n\n\n\n\nDefine the forward model, prior distribution, and likelihood function\n\n\nCompute the MAP point by solving a deterministic inverse problem\n\n\nConstruct the Laplace Approximation to the posterior distribution with a low-rank based approximation of the covariace operator.\n\n\n\n\nThe main classes and functions of \nhIPPYlib\n employed in this example are\n\n\n\n\nhippylib::PDEVariationalProblem\n : forward, adjoint and incremental problems solvers and their derivatives evaluations\n\n\nhippylib::BiLaplacianPrior\n : a biLaplacian Gaussian prior model\n\n\nhippylib::GaussianLRPosterior\n : the low rank Gaussian approximation of the posterior (used for generating starting points of MCMC simulations)\n\n\n\n\nMUQ\n is used to sample from the posterior by implementing MCMC methods with various kernels and proposals. \n\n\nThe main classes and functions used here are\n\n\n\n\npymuqModeling::PyModPiece\n : an abstract interface for defining vector-valued models\n\n\npymuqModeling::PyGaussianBase\n : an abstract interface for implementing Gaussian distributions\n\n\npymuqModeling::WorkGraph\n : a graph or a frame of connected \npymuqModeling::PyModPiece\n (or \npymuqModeling::WorkPiece\n) classes\n\n\npymuqSamplingAlgorithms::CrankNicolsonProposal\n : the pCN proposal\n\n\npymuqSamplingAlgorithms::MALAProposal\n : the MALA proposal\n\n\npymuqSamplingAlgorithms::MHKernel\n : the Metropolis-Hastings transition kernel\n\n\npymuqSamplingAlgorithms::DRKernel\n : the delayed rejection kernel\n\n\npymuqSamplingAlgorithms::SingleChainMCMC\n : a single chain MCMC sampler\n\n\n\n\nTo interface \nhIPPYlib\n and \nMUQ\n for this example, \nhippymuq\n provides the following classes:\n\n\n\n\nhippymuq::Param2LogLikelihood\n : a child of \nmuq::PyModPiece\n which wraps \nhippylib::PDEVariationalProblem\n and \nhippylib:PointwiseStateObservation\n (solving the forward problem, mapping from parameters to log likelihood and evaluating its derivative)\n\n\nhippymuq::BiLaplaceGaussian\n : a child of \npymuqModeling::PyGaussianBase\n which wraps \nhippylib::BiLaplacianPrior\n\n\nhippymuq::LAPosteriorGaussian\n : a child of \npymuqModeling::PyGaussianBase\n which wraps \nhippylib::GaussianLRPosterior\n\n\n\n\nIII. Implementation\n\n\n1. Load modules\n\n\nfrom __future__ import absolute_import, division, print_function\n\nimport math\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport dolfin as dl\nfrom hippylib import *\nimport pymuqModeling as mm\nimport pymuqSamplingAlgorithms as ms\nimport hippymuq as hm\nimport numpy as np\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\nnp.random.seed(seed=1)\n\n\n\n\n2. Generate the true parameter\n\n\nThis function generates a random field with a prescribed anisotropic covariance function.\n\n\ndef true_model(prior):\n    noise = dl.Vector()\n    prior.init_vector(noise, \"noise\")\n    parRandom.normal(1., noise)\n    mtrue = dl.Vector()\n    prior.init_vector(mtrue, 0)\n    prior.sample(noise, mtrue)\n    return mtrue\n\n\n\n\n3. Set up the mesh and finite element spaces\n\n\nWe compute a two dimensional mesh of a unit square with nx by ny elements.\nWe define a P2 finite element space for the \nstate\n and \nadjoint\n variable and P1 for the \nparameter\n.\n\n\nndim = 2\nnx   = 32\nny   = 32\nmesh = dl.UnitSquareMesh(nx, ny)\n\nVh2 = dl.FunctionSpace(mesh, 'Lagrange', 2)\nVh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\nVh  = [Vh2, Vh1, Vh2]\nprint(\"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format(\n    Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) )\n\n\n\n\nNumber of dofs: STATE=4225, PARAMETER=1089, ADJOINT=4225\n\n\n\n4. Set up the forward problem\n\n\nLet \n\\Omega\n be the unit square in \n\\mathbb{R}^2\n, and \n\\Gamma_D\n, \n\\Gamma_N\n  be the Dirichlet and Neumann portitions of the boundary \n\\partial \\Omega\n (that is \n\\Gamma_D \\cup \\Gamma_N = \\partial \\Omega\n, \n\\Gamma_D \\cap \\Gamma_N = \\emptyset\n). The forward problem reads\n\n\n\n\n\n\\left\\{\n\\begin{array}{ll}\n\\nabla \\cdot \\left( e^m \\nabla u\\right) = f & \\text{in } \\Omega\\\\\nu = u_D & \\text{on } \\Gamma_D, \\\\\ne^m \\nabla u \\cdot \\boldsymbol{n} = 0 & \\text{on } \\Gamma_N,\n\\end{array}\n\\right.\n\n\n\n\n\nwhere \nu \\in \\mathcal{V}\n is the state variable, and \nm \\in \\mathcal{M}\n is the uncertain parameter. Here \n\\Gamma_D\n corresponds to the top and bottom sides of the unit square, and \n\\Gamma_N\n corresponds to the left and right sides.\nWe also let \nf = 0\n, and \nu_D = 1\n on the top boundary and \nu_D = 0\n on the bottom boundary.\n\n\nTo set up the forward problem we use the \nPDEVariationalProblem\n class, which requires the following inputs\n- the finite element spaces for the state, parameter, and adjoint variables \nVh\n\n- the pde in weak form \npde_varf\n\n- the boundary conditions \nbc\n for the forward problem and \nbc0\n for the adjoint and incremental problems.\n\n\nThe \nPDEVariationalProblem\n class offer the following functionality:\n- solving the forward/adjoint and incremental problems\n- evaluate first and second partial derivative of the forward problem with respect to the state, parameter, and adojnt variables.\n\n\ndef u_boundary(x, on_boundary):\n    return on_boundary and ( x[1] < dl.DOLFIN_EPS or x[1] > 1.0 - dl.DOLFIN_EPS)\n\nu_bdr  = dl.Expression(\"x[1]\", degree=1)\nu_bdr0 = dl.Constant(0.0)\nbc  = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\nbc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n\nf = dl.Constant(0.0)\n\ndef pde_varf(u,m,p):\n    return dl.exp(m)*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx - f*p*dl.dx\n\npde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)\n\n\n\n\n5. Set up the prior\n\n\nTo obtain the synthetic true parameter \nm_{\\rm true}\n we generate a realization from the prior distribution.\n\n\nHere we assume a Gaussian prior, \n\\mu_{\\rm prior} \\sim \\mathcal{N}(0, \\prcov)\n with zero mean and covariance matrix \n\\prcov = \\mathcal{A}^{-2}\n, which is implemented by \nBiLaplacianPrior\n class that provides methods to apply the regularization (precision) operator to a vector or to apply the prior covariance operator.\n\n\ngamma = .1\ndelta = .5\n\ntheta0 = 2.\ntheta1 = .5\nalpha  = math.pi/4\n\nanis_diff = dl.CompiledExpression(ExpressionModule.AnisTensor2D(), degree = 1)\nanis_diff.set(theta0, theta1, alpha)\n\nprior = BiLaplacianPrior(Vh[PARAMETER], gamma, delta, anis_diff, robin_bc=True)\nprint(\"Prior regularization: (delta_x - gamma*Laplacian)^order: \"\n      \"delta={0}, gamma={1}, order={2}\".format(delta, gamma,2))\n\nmtrue = true_model(prior)\n\nobjs = [dl.Function(Vh[PARAMETER],mtrue), dl.Function(Vh[PARAMETER],prior.mean)]\nmytitles = [\"True parameter\", \"Prior mean\"]\nnb.multi1_plot(objs, mytitles)\nplt.show()\n\n\n\n\nPrior regularization: (delta_x - gamma*Laplacian)^order: delta=0.5, gamma=0.1, order=2\n\n\n\n\n\n6. Set up the likelihood and generate synthetic observations\n\n\nTo setup the observation operator \n\\mathcal{B}: \\mathcal{V} \\mapsto \\mathbb{R}^{n_t}\n, we generate \nn_t\n (\nntargets\n in the code below) random locations where to evaluate the value of the state.\n\n\nUnder the assumption of Gaussian additive noise, the likelihood function \n\\pi_{\\rm like}\n has the form\n\n\n\n\n\\pi_{\\rm like}( \\data \\,| \\, m ) \\propto \\exp\\left( -\\frac{1}{2}\\|\\mathcal{B}\\,u(m) - \\data \\|^2_{\\Gamma_{\\rm noise}^{-1}}\\right), \n\n\n\n\nwhere \nu(m)\n denotes the solution of the forward model at a given parameter \nm\n.\n\n\nThe class \nPointwiseStateObservation\n implements the evaluation of the log-likelihood function and of its partial derivatives w.r.t. the state \nu\n and parameter \nm\n.\n\n\nTo generate the synthetic observation, we first solve the forward problem using the true parameter \nm_{\\rm true}\n. Synthetic observations are obtained by perturbing the state variable at the observation points with a random Gaussian noise.\n\nrel_noise\n is the signal to noise ratio.\n\n\nntargets  = 300\nrel_noise = 0.005\ntargets   = np.random.uniform(0.05, 0.95, [ntargets, ndim])\nprint(\"Number of observation points: {0}\".format(ntargets))\n\nmisfit = PointwiseStateObservation(Vh[STATE], targets)\n\nutrue = pde.generate_state()\nx = [utrue, mtrue, None]\npde.solveFwd(x[STATE], x)\nmisfit.B.mult(x[STATE], misfit.d)\nMAX = misfit.d.norm(\"linf\")\nnoise_std_dev = rel_noise * MAX\nparRandom.normal_perturb(noise_std_dev, misfit.d)\nmisfit.noise_variance = noise_std_dev*noise_std_dev\n\nmodel = Model(pde, prior, misfit)\n\nvmax = max( utrue.max(), misfit.d.max() )\nvmin = min( utrue.min(), misfit.d.min() )\n\nplt.figure(figsize=(15,5))\nnb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True state\", \n        subplot_loc=121, vmin=vmin, vmax=vmax, cmap=\"jet\")\nnb.plot_pts(targets, misfit.d, mytitle=\"Observations\", \n            subplot_loc=122, vmin=vmin, vmax=vmax, cmap=\"jet\")\nplt.show()\n\n\n\n\nNumber of observation points: 300\n\n\n\n\n\n7. Compute the MAP point\n\n\nWe used the globalized Newtown-CG method to compute the MAP point.\n\n\nm = prior.mean.copy()\nsolver = ReducedSpaceNewtonCG(model)\nsolver.parameters[\"rel_tolerance\"]  = 1e-6\nsolver.parameters[\"abs_tolerance\"]  = 1e-12\nsolver.parameters[\"max_iter\"]       = 25\nsolver.parameters[\"GN_iter\"]        = 5\nsolver.parameters[\"globalization\"]  = \"LS\"\nsolver.parameters[\"LS\"][\"c_armijo\"] = 1e-4\n\nx = solver.solve([None, m, None])\n\nif solver.converged:\n    print( \"\\nConverged in \", solver.it, \" iterations.\")\nelse:\n    print( \"\\nNot Converged\")\n\nprint( \"Termination reason:  \", solver.termination_reasons[solver.reason] )\nprint( \"Final gradient norm: \", solver.final_grad_norm )\nprint( \"Final cost:          \", solver.final_cost )\n\nplt.figure(figsize=(18,4))\nmtrue_min = dl.Function(Vh[PARAMETER],mtrue).vector().min()\nmtrue_max = dl.Function(Vh[PARAMETER],mtrue).vector().max()\nnb.plot(dl.Function(Vh[PARAMETER],mtrue), subplot_loc=131, mytitle=\"True parameter\", \n        vmin=mtrue_min, vmax=mtrue_max)\nnb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=132,mytitle=\"MAP\", \n        vmin=mtrue_min, vmax=mtrue_max)\nnb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=133,mytitle=\"Recovered state\", cmap=\"jet\")\nplt.show()\n\n\n\n\nIt  cg_it   cost       misfit     reg        (g,dm)     ||g||L2    alpha      tolcg     \n  1     2   6.95e+03   6.94e+03   9.30e-01  -7.91e+04   1.78e+05   1.00e+00   5.00e-01\n  2     3   2.81e+03   2.81e+03   1.36e+00  -8.27e+03   5.53e+04   1.00e+00   5.00e-01\n  3     4   9.25e+02   9.21e+02   3.58e+00  -3.70e+03   2.53e+04   1.00e+00   3.77e-01\n  4    10   3.39e+02   3.30e+02   9.32e+00  -1.30e+03   9.45e+03   1.00e+00   2.30e-01\n  5     1   2.71e+02   2.61e+02   9.32e+00  -1.37e+02   1.41e+04   1.00e+00   2.81e-01\n  6    13   1.73e+02   1.56e+02   1.63e+01  -1.96e+02   3.73e+03   1.00e+00   1.45e-01\n  7    16   1.44e+02   1.20e+02   2.40e+01  -5.63e+01   1.78e+03   1.00e+00   1.00e-01\n  8    12   1.41e+02   1.16e+02   2.45e+01  -6.92e+00   1.14e+03   1.00e+00   8.01e-02\n  9    43   1.34e+02   9.87e+01   3.50e+01  -1.47e+01   8.67e+02   1.00e+00   6.98e-02\n 10     3   1.34e+02   9.86e+01   3.50e+01  -1.31e-01   3.77e+02   1.00e+00   4.60e-02\n 11    42   1.34e+02   9.85e+01   3.51e+01  -8.26e-02   8.90e+01   1.00e+00   2.24e-02\n 12    59   1.34e+02   9.85e+01   3.51e+01  -7.70e-04   8.86e+00   1.00e+00   7.06e-03\n\nConverged in  12  iterations.\nTermination reason:   Norm of the gradient less than tolerance\nFinal gradient norm:  0.10248108935885225\nFinal cost:           133.60199155509807\n\n\n\n\n\n8. Compute the low-rank based Laplace approximation of the posterior (LA-posterior)\n\n\nWe used the \ndouble pass\n algorithm to compute a low-rank decomposition of the Hessian Misfit.\nIn particular, we solve\n\n\n\n\n \\matHmis {\\bf v}_i = \\lambda_i \\Gprior^{-1} {\\bf v}_i. \n\n\n\n\nThe effective rank of the Hessian misfit is the number of eigenvalues above the red line (\ny=1\n).\nThe effective rank is independent of the mesh size.\n\n\nmodel.setPointForHessianEvaluations(x, gauss_newton_approx=False)\nHmisfit = ReducedHessian(model, misfit_only=True)\nk = 100\np = 20\nprint( \"Single/Double Pass Algorithm. Requested eigenvectors: \"\\\n       \"{0}; Oversampling {1}.\".format(k,p) )\n\nOmega = MultiVector(x[PARAMETER], k+p)\nparRandom.normal(1., Omega)\nlmbda, V = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k)\n\nnu = GaussianLRPosterior(prior, lmbda, V)\nnu.mean = x[PARAMETER]\n\nplt.plot(range(0,k), lmbda, 'b*', range(0,k+1), np.ones(k+1), '-r')\nplt.yscale('log')\nplt.xlabel('number')\nplt.ylabel('eigenvalue')\nplt.show()\n\n\n\n\nSingle/Double Pass Algorithm. Requested eigenvectors: 100; Oversampling 20.\n\n\n\n\n\n9. Drawing samples from the prior distribution and Laplace Approximation\n\n\nnsamples = 3\nnoise = dl.Vector()\nnu.init_vector(noise,\"noise\")\ns_prior = dl.Function(Vh[PARAMETER], name=\"sample_prior\")\ns_post = dl.Function(Vh[PARAMETER], name=\"sample_post\")\n\npost_pw_variance, pr_pw_variance, corr_pw_variance =\\\n    nu.pointwise_variance(method=\"Exact\")\n\npr_max   =  2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max()\npr_min   = -2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.min()\nps_max   =  2.5*math.sqrt( post_pw_variance.max() ) + nu.mean.max()\nps_min   = -2.5*math.sqrt( post_pw_variance.max() ) + nu.mean.min()\n\nfig = plt.figure(figsize=(18,8))\nfor i in range(nsamples):\n    parRandom.normal(1., noise)\n    nu.sample(noise, s_prior.vector(), s_post.vector())\n\n    impr = nb.plot(s_prior, subplot_loc=231+i, vmin=pr_min, vmax=pr_max, colorbar=None)\n    imps = nb.plot(s_post, subplot_loc=234+i, vmin=ps_min, vmax=ps_max, colorbar=None)\n\nfig.tight_layout()\nfig.subplots_adjust(left=0.15, right=0.8)\npos_impr = impr.axes.get_position().get_points()\npos_imps = imps.axes.get_position().get_points()\nheight_im = impr.axes.get_position().size[1]\ncbaxes_pr = fig.add_axes([pos_impr[1,0]+0.01, pos_impr[0,1], 0.01, height_im])\ncbaxes_ps = fig.add_axes([pos_imps[1,0]+0.01, pos_imps[0,1], 0.01, height_im])\nfig.colorbar(impr, cbaxes_pr)\nfig.colorbar(imps, cbaxes_ps)\nfig.text(0.15, pos_impr[0,1]+0.125, 'Prior samples', fontsize=20, rotation=90)\nfig.text(0.15, pos_imps[0,1]+0.1, 'Laplace samples', fontsize=20, rotation=90)\nplt.show()\n\n\n\n\n\n\n10 Define a quantify of interest\n\n\nAs a quantity of interest, we consider the log of the flux through the bottom boundary:\n\n\n\n\n q(m) = \\ln \\left\\{ \\int_{\\Gamma_b} e^m \\nabla u \\cdot \\mathbf{n} \\, ds \\right\\}, \n\n\n\n\nwhere the state variable \nu\n denotes the pressure, and \n\\mathbf{n}\n is the unit normal vector to \n\\Gamma_b\n (the bottom boundary of the domain).\n\n\nclass FluxQOI(object):\n    def __init__(self, Vh, dsGamma):\n        self.Vh = Vh\n        self.dsGamma = dsGamma\n        self.n = dl.Constant((0.,1.))\n\n        self.u = None\n        self.m = None\n        self.L = {}\n\n    def form(self, x):\n        return dl.exp(x[PARAMETER])*dl.dot( dl.grad(x[STATE]), self.n)*self.dsGamma\n\n    def eval(self, x):\n        u = vector2Function(x[STATE], self.Vh[STATE])\n        m = vector2Function(x[PARAMETER], self.Vh[PARAMETER])\n        return np.log( dl.assemble(self.form([u,m])) )\n\nclass GammaBottom(dl.SubDomain):\n    def inside(self, x, on_boundary):\n        return ( abs(x[1]) < dl.DOLFIN_EPS )\n\nGC = GammaBottom()\nmarker = dl.MeshFunction(\"size_t\", mesh, 1)\nmarker.set_all(0)\nGC.mark(marker, 1)\ndss = dl.Measure(\"ds\", subdomain_data=marker)\nqoi = FluxQOI(Vh,dss(1))\n\n\n\n\n11. Exploring the posterior using MCMC methods\n\n\nDefine the parameter-to-observable map in MUQ\n\n\nOverall, we want a mapping from parameter coefficients vector to the log target, \nJ(m) = - \\tfrac{1}{2} \\parallel {\\bf f}(m) - \\data \\parallel^{2}_{{\\bf \\Gamma}_{\\text{noise}}^{-1}} \\! - \\tfrac{1}{2}\\parallel m - m_{\\rm prior} \\parallel^{2}_{\\prcov^{-1}}\n.\nTo do so, we generate a MUQ WorkGraph of connected ModPieces.\n\n\n# a place holder ModPiece for the parameters\nidparam = mm.IdentityOperator(Vh[PARAMETER].dim())\n\n# log Gaussian Prior ModPiece\ngaussprior = hm.BiLaplaceGaussian(prior)\nlog_gaussprior = gaussprior.AsDensity()\n\n# parameter to log likelihood Modpiece\nparam2loglikelihood = hm.Param2LogLikelihood(pde, misfit)\n\n# log target ModPiece\nlog_target = mm.DensityProduct(2)\n\nworkgraph = mm.WorkGraph()\n\n# Identity operator for the parameters\nworkgraph.AddNode(idparam, 'Identity')\n\n# Prior model\nworkgraph.AddNode(log_gaussprior, \"Log_prior\")\n\n# Likelihood model\nworkgraph.AddNode(param2loglikelihood, \"Log_likelihood\")\n\n# Posterior\nworkgraph.AddNode(log_target, \"Log_target\")\n\nworkgraph.AddEdge(\"Identity\", 0, \"Log_prior\", 0)\nworkgraph.AddEdge(\"Log_prior\", 0, \"Log_target\", 0)\n\nworkgraph.AddEdge(\"Identity\", 0, \"Log_likelihood\", 0)\nworkgraph.AddEdge(\"Log_likelihood\", 0, \"Log_target\", 1)\n\nworkgraph.Visualize(\"workgraph.png\")\n\n# Construct the problem\nproblem = ms.SamplingProblem(workgraph.CreateModPiece(\"Log_target\"))\n\n\n\n\n\n\nSet up MCMC methods\n\n\nWe run five different MCMC methods:\n\n\n\n\npCN\n: Metropolis-Hastings kernel + pCN proposal with \nm_{\\rm prop} = m_{\\rm prior} = 0\n and \n\\mathcal{C}_{\\rm prop} = \\prcov\n.\n\n\nMALA\n: Metropolis-Hastings kernel + MALA proposal with \n\\mathcal{A}_{\\rm prop} = \\prcov\n\n\n\n\nh-pCN\n: Metropolis-Hastings kernel + pCN proposal with \nm_{\\rm prop} = \\map\n and \n\\mathcal{C}_{\\rm prop} = \\postcov\n\n\n\n\nh-MALA\n: Metropolis-Hastings kernel + MALA proposal with \n\\mathcal{A}_{\\rm prop} = \\postcov\n\n\n\n\nDR (h-pCN/h-MALA)\n: Delayed rejection kernel + two stage proposals (h-pCN proposal as first stage and h-MALA proposal as second stage)\n\n\n\n\nwhere \n\\postcov\n is the covariance of the LA-posterior.\n\n\nWe set the value of parameters (\n\\beta\n for pCN and \n\\tau\n for MALA) such that the acceptance rates are about 20-35% and 50-60% for pCN and MALA, respectively.\n\n\n# Construct options for MH kernel and MCMC sampler\noptions = dict()\noptions['NumSamples'] = 22000  # Number of MCMC steps to take\noptions['BurnIn'] = 2000  # Number of steps to throw away as burn in\noptions['PrintLevel'] = 0  # in {0,1,2,3} Verbosity of the output\n\nmethod_list = dict()\n\n# pCN\nopts = options.copy()\nopts.update( {'Beta':0.005} )\ngauss_pcn = hm.BiLaplaceGaussian(prior)\nprop = ms.CrankNicolsonProposal(opts, problem, gauss_pcn)\nkern = ms.MHKernel(opts, problem, prop)\nsampler = ms.SingleChainMCMC(opts, [kern])\n\nmethod_list['pCN'] = {'Options': opts, 'Sampler': sampler}\n\n# MALA\nopts = options.copy()\nopts.update( {'StepSize':0.000006} )\ngauss_mala = hm.BiLaplaceGaussian(prior, use_zero_mean=True)\nprop = ms.MALAProposal(opts, problem, gauss_mala)\nkern = ms.MHKernel(opts, problem, prop)\nsampler = ms.SingleChainMCMC(opts, [kern])\n\nmethod_list['MALA'] = {'Options': opts, 'Sampler': sampler}\n\n# h-pCN\nopts = options.copy()\nopts.update( {'Beta':0.55} )\ngauss_hpcn = hm.LAPosteriorGaussian(nu)\nprop = ms.CrankNicolsonProposal(opts, problem, gauss_hpcn)\nkern = ms.MHKernel(opts, problem, prop)\nsampler = ms.SingleChainMCMC(opts, [kern])\n\nmethod_list['h-pCN'] = {'Options': opts, 'Sampler': sampler}\n\n# h-MALA\nopts = options.copy()\nopts.update( {'StepSize':0.1} )\ngauss_hmala = hm.LAPosteriorGaussian(nu, use_zero_mean=True)\nprop = ms.MALAProposal(opts, problem, gauss_hmala)\nkern = ms.MHKernel(opts, problem, prop)\nsampler = ms.SingleChainMCMC(opts, [kern])\n\nmethod_list['h-MALA'] = {'Options': opts, 'Sampler': sampler}\n\n# DR (h-pCN/h-MALA)\nopts = options.copy()\nopts.update( {'Beta':1.0, 'StepSize':0.1} )\ngauss_dr1 = hm.LAPosteriorGaussian(nu)\ngauss_dr2 = hm.LAPosteriorGaussian(nu, use_zero_mean=True)\nprop1 = ms.CrankNicolsonProposal(opts, problem, gauss_dr1)\nprop2 = ms.MALAProposal(opts, problem, gauss_dr2)\nkern = ms.DRKernel( opts, problem, [prop1, prop2], [1.0, 1.0] )\nsampler = ms.SingleChainMCMC(opts, [kern])\n\nmethod_list['DR (h-pCN/h-MALA)'] = {'Options': opts, 'Sampler': sampler}\n\nhm.print_methodDict(method_list)\n\n\n\n\nMethod             Kernel     Proposal   Beta or Step-size\n----------------------------------------------------------\npCN                mh         pcn           5.0e-03\nMALA               mh         mala          6.0e-06\nh-pCN              mh         pcn           5.5e-01\nh-MALA             mh         mala          1.0e-01\nDR (h-pCN/h-MALA)  dr         pcn           1.0e+00\n                              mala          1.0e-01\n\n\n\nRun MCMC methods\n\n\n# Generate starting sample vector for all the MCMC simulations\nnoise = dl.Vector()\nnu.init_vector(noise, \"noise\")\nparRandom.normal(1., noise)\npr_s = model.generate_vector(PARAMETER)\npost_s = model.generate_vector(PARAMETER)\nnu.sample(noise, pr_s, post_s, add_mean=True)\nx0 = hm.dfVector2npArray(post_s)\n\n# Implement MCMC simulations\nfor mName, method in method_list.items():\n    # Run the MCMC sampler\n    sampler = method['Sampler']\n    samps = sampler.Run([x0])\n\n    # Save the computed results\n    method['Samples'] = samps\n    method['ElapsedTime'] = sampler.TotalTime()\n\n    kernel = sampler.Kernels()[0]\n    if \"AcceptanceRate\" in dir(kernel):\n        method['AcceptRate'] = kernel.AcceptanceRate()\n    elif \"AcceptanceRates\" in dir(kernel):\n        method['AcceptRate'] = kernel.AcceptanceRates()\n\n    print(\"Drawn \", options['NumSamples'] - options['BurnIn'] + 1, \n          \"MCMC samples using\", mName)\n\nprint(\"\\n\")\nprint(\"Parameter space dimension:\", Vh[PARAMETER].dim())\nprint(\"Number of samples:\", options['NumSamples'] - options['BurnIn'] + 1)\n\n# Keep track of the quantity of interest\nqoi_dataset = hm.track_qoiTracer(pde, qoi, method_list)\nhm.print_qoiResult(method_list, qoi_dataset)\nhm.plot_qoiResult(method_list, qoi_dataset, max_lag=300)\n\n\n\n\nDrawn  20001 MCMC samples using pCN\nDrawn  20001 MCMC samples using MALA\nDrawn  20001 MCMC samples using h-pCN\nDrawn  20001 MCMC samples using h-MALA\nDrawn  20001 MCMC samples using DR (h-pCN/h-MALA)\n\n\nParameter space dimension: 1089\nNumber of samples: 20001\n\n===================================================\n Summary of convergence diagnostics (single chain) \n===================================================\n\nMethod             E[QOI]   AR      ESS  ES/min\n-----------------------------------------------\npCN                -1.006  0.252    8.8    0.6\nMALA               -0.844  0.550    5.7    0.1\nh-pCN               0.491  0.258  191.1   12.8\nh-MALA              0.553  0.543  305.3    5.4\nDR (h-pCN/h-MALA)   0.505  0.582  550.8    7.8\n\n\n\n\n\nCopyright \u00a9 2020, Army Corps of Engineers, Massachusetts Institute of Technology, University of California--Merced, The University of Texas at Austin, Washington University in St. Louis\n\nAll Rights reserved.\n\n\nAcknowledgment\n: This work is supported by the National Science Foundation under grants ACI-1550487, ACI-1550547, and ACI-1550593.",
            "title": "Tutorial"
        },
        {
            "location": "/tutorial/#bayesian-quantification-of-parameter-uncertainty",
            "text": "",
            "title": "Bayesian quantification of parameter uncertainty"
        },
        {
            "location": "/tutorial/#i-estimating-the-posterior-pdf-of-the-coefficient-parameter-field-in-an-elliptic-pde",
            "text": "In this example we tackle the problem of quantifying the\nuncertainty in the solution of an inverse problem governed by an\nelliptic PDE via the Bayesian inference framework. \nHence, we state the inverse problem as a\nproblem of statistical inference over the space of uncertain\nparameters, which are to be inferred from data and a physical\nmodel.  The resulting solution to the statistical inverse problem\nis a posterior distribution that assigns to any candidate set of\nparameter fields, our belief (expressed as a probability) that a\nmember of this candidate set is the \"true\" parameter field that\ngave rise to the observed data.",
            "title": "I. Estimating the posterior pdf of the coefficient parameter field in an elliptic PDE"
        },
        {
            "location": "/tutorial/#ii-hippylib-muq-integration",
            "text": "The main objective of this example is to illustrate the interface between  hIPPYlib  and  MUQ .  We make use of  hIPPYlib  to   Define the forward model, prior distribution, and likelihood function  Compute the MAP point by solving a deterministic inverse problem  Construct the Laplace Approximation to the posterior distribution with a low-rank based approximation of the covariace operator.   The main classes and functions of  hIPPYlib  employed in this example are   hippylib::PDEVariationalProblem  : forward, adjoint and incremental problems solvers and their derivatives evaluations  hippylib::BiLaplacianPrior  : a biLaplacian Gaussian prior model  hippylib::GaussianLRPosterior  : the low rank Gaussian approximation of the posterior (used for generating starting points of MCMC simulations)   MUQ  is used to sample from the posterior by implementing MCMC methods with various kernels and proposals.   The main classes and functions used here are   pymuqModeling::PyModPiece  : an abstract interface for defining vector-valued models  pymuqModeling::PyGaussianBase  : an abstract interface for implementing Gaussian distributions  pymuqModeling::WorkGraph  : a graph or a frame of connected  pymuqModeling::PyModPiece  (or  pymuqModeling::WorkPiece ) classes  pymuqSamplingAlgorithms::CrankNicolsonProposal  : the pCN proposal  pymuqSamplingAlgorithms::MALAProposal  : the MALA proposal  pymuqSamplingAlgorithms::MHKernel  : the Metropolis-Hastings transition kernel  pymuqSamplingAlgorithms::DRKernel  : the delayed rejection kernel  pymuqSamplingAlgorithms::SingleChainMCMC  : a single chain MCMC sampler   To interface  hIPPYlib  and  MUQ  for this example,  hippymuq  provides the following classes:   hippymuq::Param2LogLikelihood  : a child of  muq::PyModPiece  which wraps  hippylib::PDEVariationalProblem  and  hippylib:PointwiseStateObservation  (solving the forward problem, mapping from parameters to log likelihood and evaluating its derivative)  hippymuq::BiLaplaceGaussian  : a child of  pymuqModeling::PyGaussianBase  which wraps  hippylib::BiLaplacianPrior  hippymuq::LAPosteriorGaussian  : a child of  pymuqModeling::PyGaussianBase  which wraps  hippylib::GaussianLRPosterior",
            "title": "II. hIPPYlib-MUQ integration"
        },
        {
            "location": "/tutorial/#iii-implementation",
            "text": "",
            "title": "III. Implementation"
        },
        {
            "location": "/tutorial/#1-load-modules",
            "text": "from __future__ import absolute_import, division, print_function\n\nimport math\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport dolfin as dl\nfrom hippylib import *\nimport pymuqModeling as mm\nimport pymuqSamplingAlgorithms as ms\nimport hippymuq as hm\nimport numpy as np\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\nnp.random.seed(seed=1)",
            "title": "1. Load modules"
        },
        {
            "location": "/tutorial/#2-generate-the-true-parameter",
            "text": "This function generates a random field with a prescribed anisotropic covariance function.  def true_model(prior):\n    noise = dl.Vector()\n    prior.init_vector(noise, \"noise\")\n    parRandom.normal(1., noise)\n    mtrue = dl.Vector()\n    prior.init_vector(mtrue, 0)\n    prior.sample(noise, mtrue)\n    return mtrue",
            "title": "2. Generate the true parameter"
        },
        {
            "location": "/tutorial/#3-set-up-the-mesh-and-finite-element-spaces",
            "text": "We compute a two dimensional mesh of a unit square with nx by ny elements.\nWe define a P2 finite element space for the  state  and  adjoint  variable and P1 for the  parameter .  ndim = 2\nnx   = 32\nny   = 32\nmesh = dl.UnitSquareMesh(nx, ny)\n\nVh2 = dl.FunctionSpace(mesh, 'Lagrange', 2)\nVh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\nVh  = [Vh2, Vh1, Vh2]\nprint(\"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format(\n    Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) )  Number of dofs: STATE=4225, PARAMETER=1089, ADJOINT=4225",
            "title": "3. Set up the mesh and finite element spaces"
        },
        {
            "location": "/tutorial/#4-set-up-the-forward-problem",
            "text": "Let  \\Omega  be the unit square in  \\mathbb{R}^2 , and  \\Gamma_D ,  \\Gamma_N   be the Dirichlet and Neumann portitions of the boundary  \\partial \\Omega  (that is  \\Gamma_D \\cup \\Gamma_N = \\partial \\Omega ,  \\Gamma_D \\cap \\Gamma_N = \\emptyset ). The forward problem reads   \n\\left\\{\n\\begin{array}{ll}\n\\nabla \\cdot \\left( e^m \\nabla u\\right) = f & \\text{in } \\Omega\\\\\nu = u_D & \\text{on } \\Gamma_D, \\\\\ne^m \\nabla u \\cdot \\boldsymbol{n} = 0 & \\text{on } \\Gamma_N,\n\\end{array}\n\\right.   where  u \\in \\mathcal{V}  is the state variable, and  m \\in \\mathcal{M}  is the uncertain parameter. Here  \\Gamma_D  corresponds to the top and bottom sides of the unit square, and  \\Gamma_N  corresponds to the left and right sides.\nWe also let  f = 0 , and  u_D = 1  on the top boundary and  u_D = 0  on the bottom boundary.  To set up the forward problem we use the  PDEVariationalProblem  class, which requires the following inputs\n- the finite element spaces for the state, parameter, and adjoint variables  Vh \n- the pde in weak form  pde_varf \n- the boundary conditions  bc  for the forward problem and  bc0  for the adjoint and incremental problems.  The  PDEVariationalProblem  class offer the following functionality:\n- solving the forward/adjoint and incremental problems\n- evaluate first and second partial derivative of the forward problem with respect to the state, parameter, and adojnt variables.  def u_boundary(x, on_boundary):\n    return on_boundary and ( x[1] < dl.DOLFIN_EPS or x[1] > 1.0 - dl.DOLFIN_EPS)\n\nu_bdr  = dl.Expression(\"x[1]\", degree=1)\nu_bdr0 = dl.Constant(0.0)\nbc  = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\nbc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n\nf = dl.Constant(0.0)\n\ndef pde_varf(u,m,p):\n    return dl.exp(m)*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx - f*p*dl.dx\n\npde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)",
            "title": "4. Set up the forward problem"
        },
        {
            "location": "/tutorial/#5-set-up-the-prior",
            "text": "To obtain the synthetic true parameter  m_{\\rm true}  we generate a realization from the prior distribution.  Here we assume a Gaussian prior,  \\mu_{\\rm prior} \\sim \\mathcal{N}(0, \\prcov)  with zero mean and covariance matrix  \\prcov = \\mathcal{A}^{-2} , which is implemented by  BiLaplacianPrior  class that provides methods to apply the regularization (precision) operator to a vector or to apply the prior covariance operator.  gamma = .1\ndelta = .5\n\ntheta0 = 2.\ntheta1 = .5\nalpha  = math.pi/4\n\nanis_diff = dl.CompiledExpression(ExpressionModule.AnisTensor2D(), degree = 1)\nanis_diff.set(theta0, theta1, alpha)\n\nprior = BiLaplacianPrior(Vh[PARAMETER], gamma, delta, anis_diff, robin_bc=True)\nprint(\"Prior regularization: (delta_x - gamma*Laplacian)^order: \"\n      \"delta={0}, gamma={1}, order={2}\".format(delta, gamma,2))\n\nmtrue = true_model(prior)\n\nobjs = [dl.Function(Vh[PARAMETER],mtrue), dl.Function(Vh[PARAMETER],prior.mean)]\nmytitles = [\"True parameter\", \"Prior mean\"]\nnb.multi1_plot(objs, mytitles)\nplt.show()  Prior regularization: (delta_x - gamma*Laplacian)^order: delta=0.5, gamma=0.1, order=2",
            "title": "5. Set up the prior"
        },
        {
            "location": "/tutorial/#6-set-up-the-likelihood-and-generate-synthetic-observations",
            "text": "To setup the observation operator  \\mathcal{B}: \\mathcal{V} \\mapsto \\mathbb{R}^{n_t} , we generate  n_t  ( ntargets  in the code below) random locations where to evaluate the value of the state.  Under the assumption of Gaussian additive noise, the likelihood function  \\pi_{\\rm like}  has the form   \\pi_{\\rm like}( \\data \\,| \\, m ) \\propto \\exp\\left( -\\frac{1}{2}\\|\\mathcal{B}\\,u(m) - \\data \\|^2_{\\Gamma_{\\rm noise}^{-1}}\\right),    where  u(m)  denotes the solution of the forward model at a given parameter  m .  The class  PointwiseStateObservation  implements the evaluation of the log-likelihood function and of its partial derivatives w.r.t. the state  u  and parameter  m .  To generate the synthetic observation, we first solve the forward problem using the true parameter  m_{\\rm true} . Synthetic observations are obtained by perturbing the state variable at the observation points with a random Gaussian noise. rel_noise  is the signal to noise ratio.  ntargets  = 300\nrel_noise = 0.005\ntargets   = np.random.uniform(0.05, 0.95, [ntargets, ndim])\nprint(\"Number of observation points: {0}\".format(ntargets))\n\nmisfit = PointwiseStateObservation(Vh[STATE], targets)\n\nutrue = pde.generate_state()\nx = [utrue, mtrue, None]\npde.solveFwd(x[STATE], x)\nmisfit.B.mult(x[STATE], misfit.d)\nMAX = misfit.d.norm(\"linf\")\nnoise_std_dev = rel_noise * MAX\nparRandom.normal_perturb(noise_std_dev, misfit.d)\nmisfit.noise_variance = noise_std_dev*noise_std_dev\n\nmodel = Model(pde, prior, misfit)\n\nvmax = max( utrue.max(), misfit.d.max() )\nvmin = min( utrue.min(), misfit.d.min() )\n\nplt.figure(figsize=(15,5))\nnb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True state\", \n        subplot_loc=121, vmin=vmin, vmax=vmax, cmap=\"jet\")\nnb.plot_pts(targets, misfit.d, mytitle=\"Observations\", \n            subplot_loc=122, vmin=vmin, vmax=vmax, cmap=\"jet\")\nplt.show()  Number of observation points: 300",
            "title": "6. Set up the likelihood and generate synthetic observations"
        },
        {
            "location": "/tutorial/#7-compute-the-map-point",
            "text": "We used the globalized Newtown-CG method to compute the MAP point.  m = prior.mean.copy()\nsolver = ReducedSpaceNewtonCG(model)\nsolver.parameters[\"rel_tolerance\"]  = 1e-6\nsolver.parameters[\"abs_tolerance\"]  = 1e-12\nsolver.parameters[\"max_iter\"]       = 25\nsolver.parameters[\"GN_iter\"]        = 5\nsolver.parameters[\"globalization\"]  = \"LS\"\nsolver.parameters[\"LS\"][\"c_armijo\"] = 1e-4\n\nx = solver.solve([None, m, None])\n\nif solver.converged:\n    print( \"\\nConverged in \", solver.it, \" iterations.\")\nelse:\n    print( \"\\nNot Converged\")\n\nprint( \"Termination reason:  \", solver.termination_reasons[solver.reason] )\nprint( \"Final gradient norm: \", solver.final_grad_norm )\nprint( \"Final cost:          \", solver.final_cost )\n\nplt.figure(figsize=(18,4))\nmtrue_min = dl.Function(Vh[PARAMETER],mtrue).vector().min()\nmtrue_max = dl.Function(Vh[PARAMETER],mtrue).vector().max()\nnb.plot(dl.Function(Vh[PARAMETER],mtrue), subplot_loc=131, mytitle=\"True parameter\", \n        vmin=mtrue_min, vmax=mtrue_max)\nnb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=132,mytitle=\"MAP\", \n        vmin=mtrue_min, vmax=mtrue_max)\nnb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=133,mytitle=\"Recovered state\", cmap=\"jet\")\nplt.show()  It  cg_it   cost       misfit     reg        (g,dm)     ||g||L2    alpha      tolcg     \n  1     2   6.95e+03   6.94e+03   9.30e-01  -7.91e+04   1.78e+05   1.00e+00   5.00e-01\n  2     3   2.81e+03   2.81e+03   1.36e+00  -8.27e+03   5.53e+04   1.00e+00   5.00e-01\n  3     4   9.25e+02   9.21e+02   3.58e+00  -3.70e+03   2.53e+04   1.00e+00   3.77e-01\n  4    10   3.39e+02   3.30e+02   9.32e+00  -1.30e+03   9.45e+03   1.00e+00   2.30e-01\n  5     1   2.71e+02   2.61e+02   9.32e+00  -1.37e+02   1.41e+04   1.00e+00   2.81e-01\n  6    13   1.73e+02   1.56e+02   1.63e+01  -1.96e+02   3.73e+03   1.00e+00   1.45e-01\n  7    16   1.44e+02   1.20e+02   2.40e+01  -5.63e+01   1.78e+03   1.00e+00   1.00e-01\n  8    12   1.41e+02   1.16e+02   2.45e+01  -6.92e+00   1.14e+03   1.00e+00   8.01e-02\n  9    43   1.34e+02   9.87e+01   3.50e+01  -1.47e+01   8.67e+02   1.00e+00   6.98e-02\n 10     3   1.34e+02   9.86e+01   3.50e+01  -1.31e-01   3.77e+02   1.00e+00   4.60e-02\n 11    42   1.34e+02   9.85e+01   3.51e+01  -8.26e-02   8.90e+01   1.00e+00   2.24e-02\n 12    59   1.34e+02   9.85e+01   3.51e+01  -7.70e-04   8.86e+00   1.00e+00   7.06e-03\n\nConverged in  12  iterations.\nTermination reason:   Norm of the gradient less than tolerance\nFinal gradient norm:  0.10248108935885225\nFinal cost:           133.60199155509807",
            "title": "7. Compute the MAP point"
        },
        {
            "location": "/tutorial/#8-compute-the-low-rank-based-laplace-approximation-of-the-posterior-la-posterior",
            "text": "We used the  double pass  algorithm to compute a low-rank decomposition of the Hessian Misfit.\nIn particular, we solve    \\matHmis {\\bf v}_i = \\lambda_i \\Gprior^{-1} {\\bf v}_i.    The effective rank of the Hessian misfit is the number of eigenvalues above the red line ( y=1 ).\nThe effective rank is independent of the mesh size.  model.setPointForHessianEvaluations(x, gauss_newton_approx=False)\nHmisfit = ReducedHessian(model, misfit_only=True)\nk = 100\np = 20\nprint( \"Single/Double Pass Algorithm. Requested eigenvectors: \"\\\n       \"{0}; Oversampling {1}.\".format(k,p) )\n\nOmega = MultiVector(x[PARAMETER], k+p)\nparRandom.normal(1., Omega)\nlmbda, V = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k)\n\nnu = GaussianLRPosterior(prior, lmbda, V)\nnu.mean = x[PARAMETER]\n\nplt.plot(range(0,k), lmbda, 'b*', range(0,k+1), np.ones(k+1), '-r')\nplt.yscale('log')\nplt.xlabel('number')\nplt.ylabel('eigenvalue')\nplt.show()  Single/Double Pass Algorithm. Requested eigenvectors: 100; Oversampling 20.",
            "title": "8. Compute the low-rank based Laplace approximation of the posterior (LA-posterior)"
        },
        {
            "location": "/tutorial/#9-drawing-samples-from-the-prior-distribution-and-laplace-approximation",
            "text": "nsamples = 3\nnoise = dl.Vector()\nnu.init_vector(noise,\"noise\")\ns_prior = dl.Function(Vh[PARAMETER], name=\"sample_prior\")\ns_post = dl.Function(Vh[PARAMETER], name=\"sample_post\")\n\npost_pw_variance, pr_pw_variance, corr_pw_variance =\\\n    nu.pointwise_variance(method=\"Exact\")\n\npr_max   =  2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max()\npr_min   = -2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.min()\nps_max   =  2.5*math.sqrt( post_pw_variance.max() ) + nu.mean.max()\nps_min   = -2.5*math.sqrt( post_pw_variance.max() ) + nu.mean.min()\n\nfig = plt.figure(figsize=(18,8))\nfor i in range(nsamples):\n    parRandom.normal(1., noise)\n    nu.sample(noise, s_prior.vector(), s_post.vector())\n\n    impr = nb.plot(s_prior, subplot_loc=231+i, vmin=pr_min, vmax=pr_max, colorbar=None)\n    imps = nb.plot(s_post, subplot_loc=234+i, vmin=ps_min, vmax=ps_max, colorbar=None)\n\nfig.tight_layout()\nfig.subplots_adjust(left=0.15, right=0.8)\npos_impr = impr.axes.get_position().get_points()\npos_imps = imps.axes.get_position().get_points()\nheight_im = impr.axes.get_position().size[1]\ncbaxes_pr = fig.add_axes([pos_impr[1,0]+0.01, pos_impr[0,1], 0.01, height_im])\ncbaxes_ps = fig.add_axes([pos_imps[1,0]+0.01, pos_imps[0,1], 0.01, height_im])\nfig.colorbar(impr, cbaxes_pr)\nfig.colorbar(imps, cbaxes_ps)\nfig.text(0.15, pos_impr[0,1]+0.125, 'Prior samples', fontsize=20, rotation=90)\nfig.text(0.15, pos_imps[0,1]+0.1, 'Laplace samples', fontsize=20, rotation=90)\nplt.show()",
            "title": "9. Drawing samples from the prior distribution and Laplace Approximation"
        },
        {
            "location": "/tutorial/#10-define-a-quantify-of-interest",
            "text": "As a quantity of interest, we consider the log of the flux through the bottom boundary:    q(m) = \\ln \\left\\{ \\int_{\\Gamma_b} e^m \\nabla u \\cdot \\mathbf{n} \\, ds \\right\\},    where the state variable  u  denotes the pressure, and  \\mathbf{n}  is the unit normal vector to  \\Gamma_b  (the bottom boundary of the domain).  class FluxQOI(object):\n    def __init__(self, Vh, dsGamma):\n        self.Vh = Vh\n        self.dsGamma = dsGamma\n        self.n = dl.Constant((0.,1.))\n\n        self.u = None\n        self.m = None\n        self.L = {}\n\n    def form(self, x):\n        return dl.exp(x[PARAMETER])*dl.dot( dl.grad(x[STATE]), self.n)*self.dsGamma\n\n    def eval(self, x):\n        u = vector2Function(x[STATE], self.Vh[STATE])\n        m = vector2Function(x[PARAMETER], self.Vh[PARAMETER])\n        return np.log( dl.assemble(self.form([u,m])) )\n\nclass GammaBottom(dl.SubDomain):\n    def inside(self, x, on_boundary):\n        return ( abs(x[1]) < dl.DOLFIN_EPS )\n\nGC = GammaBottom()\nmarker = dl.MeshFunction(\"size_t\", mesh, 1)\nmarker.set_all(0)\nGC.mark(marker, 1)\ndss = dl.Measure(\"ds\", subdomain_data=marker)\nqoi = FluxQOI(Vh,dss(1))",
            "title": "10 Define a quantify of interest"
        },
        {
            "location": "/tutorial/#11-exploring-the-posterior-using-mcmc-methods",
            "text": "",
            "title": "11. Exploring the posterior using MCMC methods"
        },
        {
            "location": "/research/",
            "text": "Selected publications\n\n\n\n\n\n\nK. Koval, A. Alexanderian, G. Stadler,\n\nOptimal experimental design under irreducible uncertainty for inverse problems governed by PDEs\n,\narXiv, 2019\n\n\n\n\n\n\nS. Wahal, G. Biros,\n\nBIMC: The Bayesian Inverse Monte Carlo method for goal-oriented uncertainty quantification. Part I\n,\narXiv, 2019\n\n\n\n\n\n\nC. Dwelle, J. Kim, K. Sargsyan, V. Ivanov,\n*\nStreamflow, stomata, and soil pits: Sources of inference for complex models with fast, robust uncertainty quantification\n,\nIn: Advances in Water Resources, 125, 2019\n\n\n\n\n\n\nS. Kramer, A. Jones, A. Mostafa, et al.\n\nThe third Sandia fracture challenge: predictions of ductile fracture in additively manufactured metal\n,\n Int J Fract 218, 5\u201361, 2019\n\n\n\n\n\n\nS. Lan, \nAdaptive dimension reduction to accelerate infinite-dimensional geometric Markov Chain Monte Carlo\n,\nJournal of Computational Physics, 392:71-95, 2019\n\n\n\n\n\n\nU. Villa, N. Petra, O. Ghattas,\n\nhIPPYlib: An extensible software framework for large-scale inverse problems; Part I: Deterministic inversion and linearized Bayesian inference\n,\narXiv, 2019\n\n\n\n\n\n\nP. Chen, U. Villa, O. Ghattas,\n\nTaylor approximation and variance reduction for PDE-constrained optimal control under uncertainty\n,\nJournal of Computational Physics, 385:163--186, 2019\n\n\n\n\n\n\nM. Parno, D. O'Connor, M. Smith,\n\nHigh dimensional inference for the structural health monitoring of lock gates\n,\narXiv, 2018\n\n\n\n\n\n\nB. Crestel, G. Stadler and O. Ghattas,\n\nA comparative study of structural similarity and regularization for joint inverse problems governed by PDEs\n,\nInverse Problems, 35:024003, 2018\n\n\n\n\n\n\nA. Attia, A. Alexanderian, A. K. Saibaba,\n\nGoal-oriented optimal design of experiments for large-scale Bayesian linear inverse problems\n,\nInverse Problems, 34:095009, 2018\n\n\n\n\n\n\nR. Nicholson, N. Petra and Jari P Kaipio,\n\nEstimation of the Robin coefficient field in a Poisson problem with uncertain conductivity field\n,\nInverse Problems, Volume 34, Number 11, 2018\n\n\n\n\n\n\nP. Chen, K. Wu, J. Chen, T. O'Leary-Roseberry, O. Ghattas,\n\nProjected Stein Variational Newton: A Fast and Scalable Bayesian Inference Method in High Dimensions\n,\narXiv, 2018\n\n\n\n\n\n\nE. M. Constantinescu, N. Petra, J. Bessac, C. G. Petra,\n\nStatistical Treatment of Inverse Problems Constrained by Differential Equations-Based Models with Stochastic Terms\n,\narXiv, 2018\n\n\n\n\n\n\nU. Villa, N. Petra, O. Ghattas,\n\nhIPPYlib: An extensible software framework for large-scale inverse problems\n,\nJournal of Open Source Software (JOSS), 3(30):940, 2018\n\n\n\n\n\n\nP. Chen, U. Villa, O. Ghattas,\n\nTaylor approximation for PDE-constrained optimization under uncertainty: Application to turbulent jet flow\n,\nProceedings in Applied Mathematics and Mechanics - 89th GAMM Annual Meeting, 18:e201800466, 2018\n\n\n\n\n\n\nM. Parno, Y. Marzouk,\n\nTransport Map Accelerated Markov Chain Monte Carlo\n,\nSIAM/ASA J. Uncertainty Quantification, 6(2), 645\u2013682, 2018\n\n\n\n\n\n\nP. Conrad, A. Davis, Y. Marzouk, N. S. Pillai, A. Smith,\n\nParallel Local Approximation MCMC for Expensive Models\n,\nSIAM/ASA J. Uncertainty Quantification, 6(1), 339\u2013373, 2018\n\n\n\n\n\n\nP. Chen, U. Villa, O. Ghattas,\n\nHessian-based adaptive sparse quadrature for infinite-dimensional Bayesian inverse problems\n,\nComputer Methods in Applied Mechanics and Engineering, 327:147-172, 2017\n\n\n\n\n\n\nM. Parno, T. Moselhy, Y. Marzouk,\n\nA Multiscale Strategy for Bayesian Inference Using Transport Maps\n,\nSIAM/ASA J. Uncertainty Quantification, 4(1), 1160\u20131190, 2016\n\n\n\n\n\n\nY. Marzouk, T. Moselhy, M. Parno, A. Spantini,\n\nSampling via Measure Transport: An Introduction\n,\n\nIn: Ghanem R., Higdon D., Owhadi H. (eds) Handbook of Uncertainty Quantification. Springer, 2016\n\n\n\n\n\n\nP. Conrad, Y. Marzouk, N. S. Pillai, A. Smith,\n\nAccelerating Asymptotically Exact MCMC for Computationally Intensive Models via Local Approximations\n,\nJ. American Statistical Association, 111(516), 1591-1607, 2015\n\n\n\n\n\n\nP. Conrad, Y. Marzouk,\n\nAdaptive Smolyak Pseudospectral Approximations\n,\nSIAM J. Sci. Comput., 35(6), A2643\u2013A2670, 2013\n\n\n\n\n\n\nSelected Ph.D. thesis\n\n\n\n\n\n\nS. Fatehiboroujeni, \nInverse Approaches for Identification of Constitutive Laws of Slender Structures Motivated by Application to Biological Filaments\n,\nUniversity of California, Merced, 2018. Adviser S. Goyal\n\n\n\n\n\n\nA. Davis, \nPrediction under uncertainty: from models for marine-terminating glaciers to Bayesian computation\n,\n MIT, Boston, 2015. Adviser Y. Marzouk\n\n\n\n\n\n\nK. A. McCormack, \nEarthquakes, groundwater and surface deformation: exploring the poroelastic response to megathrust earthquakes\n,\nThe University of Texas at Austin, 2018. Adviser M. Hesse\n\n\n\n\n\n\nB. Crestel, \nAdvanced techniques for multi-source, multi-parameter, and multi-physics inverse problems\n,\nThe University of Texas at Austin, 2017. Adviser O. Ghattas\n\n\n\n\n\n\nM. Parno, \nTransport maps for accelerated Bayesian computation\n,\nMIT, Boston, 2015. Adviser Y. Marzouk",
            "title": "Research"
        },
        {
            "location": "/research/#selected-publications",
            "text": "K. Koval, A. Alexanderian, G. Stadler, Optimal experimental design under irreducible uncertainty for inverse problems governed by PDEs ,\narXiv, 2019    S. Wahal, G. Biros, BIMC: The Bayesian Inverse Monte Carlo method for goal-oriented uncertainty quantification. Part I ,\narXiv, 2019    C. Dwelle, J. Kim, K. Sargsyan, V. Ivanov,\n* Streamflow, stomata, and soil pits: Sources of inference for complex models with fast, robust uncertainty quantification ,\nIn: Advances in Water Resources, 125, 2019    S. Kramer, A. Jones, A. Mostafa, et al. The third Sandia fracture challenge: predictions of ductile fracture in additively manufactured metal ,\n Int J Fract 218, 5\u201361, 2019    S. Lan,  Adaptive dimension reduction to accelerate infinite-dimensional geometric Markov Chain Monte Carlo ,\nJournal of Computational Physics, 392:71-95, 2019    U. Villa, N. Petra, O. Ghattas, hIPPYlib: An extensible software framework for large-scale inverse problems; Part I: Deterministic inversion and linearized Bayesian inference ,\narXiv, 2019    P. Chen, U. Villa, O. Ghattas, Taylor approximation and variance reduction for PDE-constrained optimal control under uncertainty ,\nJournal of Computational Physics, 385:163--186, 2019    M. Parno, D. O'Connor, M. Smith, High dimensional inference for the structural health monitoring of lock gates ,\narXiv, 2018    B. Crestel, G. Stadler and O. Ghattas, A comparative study of structural similarity and regularization for joint inverse problems governed by PDEs ,\nInverse Problems, 35:024003, 2018    A. Attia, A. Alexanderian, A. K. Saibaba, Goal-oriented optimal design of experiments for large-scale Bayesian linear inverse problems ,\nInverse Problems, 34:095009, 2018    R. Nicholson, N. Petra and Jari P Kaipio, Estimation of the Robin coefficient field in a Poisson problem with uncertain conductivity field ,\nInverse Problems, Volume 34, Number 11, 2018    P. Chen, K. Wu, J. Chen, T. O'Leary-Roseberry, O. Ghattas, Projected Stein Variational Newton: A Fast and Scalable Bayesian Inference Method in High Dimensions ,\narXiv, 2018    E. M. Constantinescu, N. Petra, J. Bessac, C. G. Petra, Statistical Treatment of Inverse Problems Constrained by Differential Equations-Based Models with Stochastic Terms ,\narXiv, 2018    U. Villa, N. Petra, O. Ghattas, hIPPYlib: An extensible software framework for large-scale inverse problems ,\nJournal of Open Source Software (JOSS), 3(30):940, 2018    P. Chen, U. Villa, O. Ghattas, Taylor approximation for PDE-constrained optimization under uncertainty: Application to turbulent jet flow ,\nProceedings in Applied Mathematics and Mechanics - 89th GAMM Annual Meeting, 18:e201800466, 2018    M. Parno, Y. Marzouk, Transport Map Accelerated Markov Chain Monte Carlo ,\nSIAM/ASA J. Uncertainty Quantification, 6(2), 645\u2013682, 2018    P. Conrad, A. Davis, Y. Marzouk, N. S. Pillai, A. Smith, Parallel Local Approximation MCMC for Expensive Models ,\nSIAM/ASA J. Uncertainty Quantification, 6(1), 339\u2013373, 2018    P. Chen, U. Villa, O. Ghattas, Hessian-based adaptive sparse quadrature for infinite-dimensional Bayesian inverse problems ,\nComputer Methods in Applied Mechanics and Engineering, 327:147-172, 2017    M. Parno, T. Moselhy, Y. Marzouk, A Multiscale Strategy for Bayesian Inference Using Transport Maps ,\nSIAM/ASA J. Uncertainty Quantification, 4(1), 1160\u20131190, 2016    Y. Marzouk, T. Moselhy, M. Parno, A. Spantini, Sampling via Measure Transport: An Introduction , \nIn: Ghanem R., Higdon D., Owhadi H. (eds) Handbook of Uncertainty Quantification. Springer, 2016    P. Conrad, Y. Marzouk, N. S. Pillai, A. Smith, Accelerating Asymptotically Exact MCMC for Computationally Intensive Models via Local Approximations ,\nJ. American Statistical Association, 111(516), 1591-1607, 2015    P. Conrad, Y. Marzouk, Adaptive Smolyak Pseudospectral Approximations ,\nSIAM J. Sci. Comput., 35(6), A2643\u2013A2670, 2013",
            "title": "Selected publications"
        },
        {
            "location": "/research/#selected-phd-thesis",
            "text": "S. Fatehiboroujeni,  Inverse Approaches for Identification of Constitutive Laws of Slender Structures Motivated by Application to Biological Filaments ,\nUniversity of California, Merced, 2018. Adviser S. Goyal    A. Davis,  Prediction under uncertainty: from models for marine-terminating glaciers to Bayesian computation ,\n MIT, Boston, 2015. Adviser Y. Marzouk    K. A. McCormack,  Earthquakes, groundwater and surface deformation: exploring the poroelastic response to megathrust earthquakes ,\nThe University of Texas at Austin, 2018. Adviser M. Hesse    B. Crestel,  Advanced techniques for multi-source, multi-parameter, and multi-physics inverse problems ,\nThe University of Texas at Austin, 2017. Adviser O. Ghattas    M. Parno,  Transport maps for accelerated Bayesian computation ,\nMIT, Boston, 2015. Adviser Y. Marzouk",
            "title": "Selected Ph.D. thesis"
        },
        {
            "location": "/outreach/",
            "text": "Outreach\n\n\nWorkshops, summer schools, and short courses\n\n\n\n\n\n\nShort course on Inverse Problems and Uncertainty Quantification\n,\nTrimester on Mathematics of Climate and the Environment,\nInstitut Henri Poincare', Paris, France, November 4\u20138, 2019\n\n\n\n\n\n\nBayesian Inverse Problems for Structural Health Monitoring of Civil Infrastructure\n,\nDartmouth College, Hanover, NH, May 21\u201324, 2019\n\n\n\n\n\n\nInverse Problems: Systematic Integration of Data with Models under Uncertainty\n,\n2018 Gene Golub SIAM Summer School, June 17-30, 2018. Breckenridge, Colorado, USA.\n\n\n\n\n\n\nSAMSI Optimization Program Summer School\n,\nResearch Triangle Park, NC,  August 8-12, 2016\n\n\n\n\n\n\nQUEST Uncertainty Quantification Summer School\n,\nUSC, 19\u201321 August, 2015\n\n\n\n\n\n\nIDEALab: Inverse Problems and Uncertainty Quantification\n,\nBrown University, Providence, RD, July 6-10, 2015\n\n\n\n\n\n\nIntroduction to Uncertainty Quantification\n,\nIMA Short Course, University of Minnesota, 15\u201326 June, 2015",
            "title": "Outreach"
        },
        {
            "location": "/outreach/#outreach",
            "text": "",
            "title": "Outreach"
        },
        {
            "location": "/outreach/#workshops-summer-schools-and-short-courses",
            "text": "Short course on Inverse Problems and Uncertainty Quantification ,\nTrimester on Mathematics of Climate and the Environment,\nInstitut Henri Poincare', Paris, France, November 4\u20138, 2019    Bayesian Inverse Problems for Structural Health Monitoring of Civil Infrastructure ,\nDartmouth College, Hanover, NH, May 21\u201324, 2019    Inverse Problems: Systematic Integration of Data with Models under Uncertainty ,\n2018 Gene Golub SIAM Summer School, June 17-30, 2018. Breckenridge, Colorado, USA.    SAMSI Optimization Program Summer School ,\nResearch Triangle Park, NC,  August 8-12, 2016    QUEST Uncertainty Quantification Summer School ,\nUSC, 19\u201321 August, 2015    IDEALab: Inverse Problems and Uncertainty Quantification ,\nBrown University, Providence, RD, July 6-10, 2015    Introduction to Uncertainty Quantification ,\nIMA Short Course, University of Minnesota, 15\u201326 June, 2015",
            "title": "Workshops, summer schools, and short courses"
        },
        {
            "location": "/about/",
            "text": "About MUQ-hIPPYlib integration\n\n\nTeam\n\n\n\n\nOmar Ghattas\n\n\nKi-Tae Kim\n\n\nYoussef Marzouk\n\n\nMatthew Parno\n\n\nNoemi Petra\n\n\nUmberto Villa\n\n\nMUQ contributors\n\n\nhIPPYlib contributors\n\n\n\n\nSupport\n\n\n\n\n\n\nO. Ghattas, Y. Marzouk, N. Petra (PIs),  M. Parno, U. Villa (Co-PIs).\nCollaborative Research: SI2-SSI: \nIntegrating Data with Complex Predictive Models under Uncertainty:\nAn Extensible Software Framework for Large-Scale Bayesian Inversion\n, NSF, ACI-1550487, ACI-1550547,\nACI-1550593 (2016-2020).\n\n\n\n\n\n\nO. Ghattas, Y. Marzouk, M. Parno, N. Petra, G. Stadler, U. Villa.\n2018 Gene Golub SIAM Summer School:\n\nSystematic Integration of Data with Models under Uncertainty\n.\nSIAM (2018).\n\n\n\n\n\n\nLicenses\n\n\n\n\nMUQ: \nBSD-3-Clause\n\n\nhIPPYlib: \nGNU General Public License version 2 (GPL)\n\n\n\n\n\n\n\n\nWebsite built with \nMkDocs\n, \nbootstrap\n, \nbootswatch\n, and \nMathJax\n.\nHosted on \nGitHub\n.",
            "title": "About"
        },
        {
            "location": "/about/#about-muq-hippylib-integration",
            "text": "",
            "title": "About MUQ-hIPPYlib integration"
        },
        {
            "location": "/about/#team",
            "text": "Omar Ghattas  Ki-Tae Kim  Youssef Marzouk  Matthew Parno  Noemi Petra  Umberto Villa  MUQ contributors  hIPPYlib contributors",
            "title": "Team"
        },
        {
            "location": "/about/#support",
            "text": "O. Ghattas, Y. Marzouk, N. Petra (PIs),  M. Parno, U. Villa (Co-PIs).\nCollaborative Research: SI2-SSI:  Integrating Data with Complex Predictive Models under Uncertainty:\nAn Extensible Software Framework for Large-Scale Bayesian Inversion , NSF, ACI-1550487, ACI-1550547,\nACI-1550593 (2016-2020).    O. Ghattas, Y. Marzouk, M. Parno, N. Petra, G. Stadler, U. Villa.\n2018 Gene Golub SIAM Summer School: Systematic Integration of Data with Models under Uncertainty .\nSIAM (2018).",
            "title": "Support"
        },
        {
            "location": "/about/#licenses",
            "text": "MUQ:  BSD-3-Clause  hIPPYlib:  GNU General Public License version 2 (GPL)     Website built with  MkDocs ,  bootstrap ,  bootswatch , and  MathJax .\nHosted on  GitHub .",
            "title": "Licenses"
        }
    ]
}