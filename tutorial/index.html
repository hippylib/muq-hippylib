<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Tutorial - MUQ-hIPPYlib</title>
        <link href="../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../css/font-awesome-4.5.0.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="../css/highlight.css">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        <script src="../js/jquery-1.10.2.min.js"></script>
        <script src="../js/bootstrap-3.0.3.min.js"></script>
        <script src="../js/highlight.pack.js"></script>
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-52448613-3', 'hippylib.github.io');
            ga('send', 'pageview');
        </script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="..">MUQ-hIPPYlib</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                    <li >
                        <a href="..">Home</a>
                    </li>
                    <li class="active">
                        <a href="./">Tutorial</a>
                    </li>
                    <li >
                        <a href="../research/">Research</a>
                    </li>
                    <li >
                        <a href="../outreach/">Outreach</a>
                    </li>
                    <li >
                        <a href="../about/">About</a>
                    </li>
                </ul>

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                    <li >
                        <a rel="next" href="..">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="../research/">
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>
            </ul>
        </div>
    </div>
</div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#bayesian-quantification-of-parameter-uncertainty">Bayesian quantification of parameter uncertainty:</a></li>
            <li><a href="#i-estimating-the-posterior-pdf-of-the-coefficient-parameter-field-in-an-elliptic-pde">I. Estimating the posterior pdf of the coefficient parameter field in an elliptic PDE</a></li>
            <li><a href="#ii-hippylib-muq-integration">II. hIPPYlib-MUQ integration</a></li>
            <li><a href="#iii-implementation">III. Implementation</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<p>
<script type="math/tex; mode=display">
\def\data{ {\bf d}_\rm{obs}}
\def\vec{\bf}
\def\m{ {\bf m}}
\def\map{m_{\nu}}
\def\postcov{ \mathcal{C}_{\nu} }
\def\prcov{ \mathcal{C}_{\text{prior}} }
\def\matrix{\bf}
\def\Hmisfit{ \mathcal{H}_{\text{misfit}} }
\def\diag{\operatorname{diag}}
\def\Vr{{\matrix V}_r}
\def\Wr{{\matrix W}_r}
\def\Ir{{\matrix I}_r}
\def\Dr{{\matrix D}_r}
\def\H{{\matrix H} }
\def\matHmis{ {\H}_{\rm misfit}}
\def\Gpost{\boldsymbol{\Gamma}_{\nu} }
\def\Gprior{ \boldsymbol{\Gamma}_{\rm prior} }
</script>
</p>
<h1 id="bayesian-quantification-of-parameter-uncertainty">Bayesian quantification of parameter uncertainty:</h1>
<h2 id="i-estimating-the-posterior-pdf-of-the-coefficient-parameter-field-in-an-elliptic-pde">I. Estimating the posterior pdf of the coefficient parameter field in an elliptic PDE</h2>
<p>In this example we tackle the problem of quantifying the
uncertainty in the solution of an inverse problem governed by an
elliptic PDE via the Bayesian inference framework. 
Hence, we state the inverse problem as a
problem of statistical inference over the space of uncertain
parameters, which are to be inferred from data and a physical
model.  The resulting solution to the statistical inverse problem
is a posterior distribution that assigns to any candidate set of
parameter fields, our belief (expressed as a probability) that a
member of this candidate set is the "true" parameter field that
gave rise to the observed data.</p>
<h4 id="bayess-theorem">Bayes's Theorem</h4>
<p>The posterior probability distribution combines the prior pdf
<script type="math/tex">\mu_{\text{prior}}(m)</script> over the parameter space, which encodes
any knowledge or assumptions about the parameter space that we may
wish to impose before the data are considered, with a likelihood pdf
<script type="math/tex">\pi_{\text{like}}(\data \; | \; m)</script>, which explicitly
represents the probability that a given parameter <script type="math/tex">m</script>
might give rise to the observed data <script type="math/tex">\data \in
\mathbb{R}^{n_t}</script>, namely:</p>
<p>
<script type="math/tex; mode=display">
\begin{align}
d \mu_{\text{post}}(m | \data) \propto \pi_{\text{like}}(\data \,|\, m) \, d\mu_{\text{prior}}(m).
\end{align}
</script>
</p>
<p>Note that infinite-dimensional analog of Bayes's formula requires the use Radon-Nikodym derivatives instead of probability density functions.</p>
<h5 id="the-prior">The prior</h5>
<p>We consider a Gaussian prior with mean <script type="math/tex">{m}_{\rm prior}</script> and covariance <script type="math/tex">\prcov</script>, <script type="math/tex">\mu_{\rm prior} \sim \mathcal{N}({m}_{\rm prior}, \prcov)</script>. The covariance is given by the discretization of the inverse of differential operator <script type="math/tex">\mathcal{A}^{-2} = (-\gamma \Delta + \delta I)^{-2}</script>, where <script type="math/tex">\gamma</script>, <script type="math/tex">\delta > 0</script> control the correlation length and the variance of the prior operator. This choice of prior ensures that it is a trace-class operator, guaranteeing bounded pointwise variance and a well-posed infinite-dimensional Bayesian inverse problem.</p>
<h5 id="the-likelihood">The likelihood</h5>
<p>
<script type="math/tex; mode=display">
\data =  {\bf f}(m) + {\bf e }, \;\;\;  {\bf e} \sim \mathcal{N}({\bf 0}, {\bf \Gamma}_{\text{noise}} )
</script>
</p>
<p>
<script type="math/tex; mode=display">
\pi_{\text like}(\data \; | \; m)  \propto \exp \left( - \tfrac{1}{2} \parallel {\bf f}(m) - \data \parallel^{2}_{{\bf \Gamma}_{\text{noise}}^{-1}}\right)
</script>
</p>
<p>Here <script type="math/tex">{\bf f}</script> is the parameter-to-observable map that takes a parameter <script type="math/tex">m</script> and maps
it to the space observation vector <script type="math/tex">\data</script>.</p>
<p>In this application, <script type="math/tex">{\bf f}</script> consists in the composition of a PDE solve (to compute the state <script type="math/tex">u</script>) and a pointwise observation of the state <script type="math/tex">u</script> to extract the observation vector <script type="math/tex">\data</script>.</p>
<h5 id="the-posterior">The posterior</h5>
<p>
<script type="math/tex; mode=display">
d\mu_{\text{post}}(m \; | \; \data)  \propto \exp \left( - \tfrac{1}{2} \parallel {\bf f}(m) - \data \parallel^{2}_{{\bf \Gamma}_{\text{noise}}^{-1}} \! - \tfrac{1}{2}\parallel m - m_{\rm prior} \parallel^{2}_{\prcov^{-1}} \right)
</script>
</p>
<h4 id="the-laplace-approximation-to-the-posterior-nu-sim-mathcalnmapbf-postcov">The Laplace approximation to the posterior: <script type="math/tex">\nu \sim \mathcal{N}({\map},\bf \postcov)</script>
</h4>
<p>The mean of the Laplace approximation posterior distribution, <script type="math/tex">{\map}</script>, is the
parameter maximizing the posterior, and
is known as the maximum a posteriori (MAP) point.  It can be found
by minimizing the negative log of the posterior, which amounts to
solving a deterministic inverse problem) with appropriately weighted norms,</p>
<p>
<script type="math/tex; mode=display">
\map := \underset{m}{\arg \min} \; \mathcal{J}(m) \;:=\;
\Big( 
\frac{1}{2} \| {\bf f}(m) - \data \|^2_{ {\bf \Gamma}_{\text{noise}}^{-1}} 
+\frac{1}{2} \| m -m_{\rm prior} \|^2_{\prcov^{-1}} 
\Big).
</script>
</p>
<p>The posterior covariance matrix is then given by the inverse of
the Hessian matrix of <script type="math/tex">\mathcal{J}</script> at <script type="math/tex">\map</script>, namely</p>
<p>
<script type="math/tex; mode=display">
\postcov = \left(\Hmisfit(\map) + \prcov^{-1} \right)^{-1},
</script>
</p>
<p>provided that <script type="math/tex">\Hmisfit(\map)</script> is positive semidefinite.</p>
<h5 id="the-generalized-eigenvalue-problem">The generalized eigenvalue problem</h5>
<p>In what follows we denote with <script type="math/tex">\matHmis, \Gpost, \Gprior \in \mathbb{R}^{n\times n}</script> the matrices stemming from the discretization of the operators <script type="math/tex">\Hmisfit(\map)</script>, <script type="math/tex">\postcov</script>, <script type="math/tex">\prcov</script> with respect to the unweighted Euclidean inner product.
Then we considered the symmetric generalized eigenvalue problem</p>
<p>
<script type="math/tex; mode=display">
 \matHmis {\matrix V} = \Gprior^{-1} {\matrix V} {\matrix \Lambda},
</script>
</p>
<p>where <script type="math/tex">{\matrix \Lambda} = \diag(\lambda_i) \in \mathbb{R}^{n\times n}</script>
contains the generalized eigenvalues and the columns of <script type="math/tex">{\matrix V}\in
\mathbb R^{n\times n}</script> the generalized eigenvectors such that 
<script type="math/tex">{\matrix V}^T \Gprior^{-1} {\matrix V} = {\matrix I}</script>.</p>
<h5 id="randomized-eigensolvers-to-construct-the-approximate-spectral-decomposition">Randomized eigensolvers to construct the approximate spectral decomposition</h5>
<p>When the generalized eigenvalues <script type="math/tex">\{\lambda_i\}</script> decay rapidly, we can
extract a low-rank approximation of <script type="math/tex">\matHmis</script> by retaining only the <script type="math/tex">r</script>
largest eigenvalues and corresponding eigenvectors,</p>
<p>
<script type="math/tex; mode=display">
 \matHmis \approx \Gprior^{-1} \Vr {\matrix{\Lambda}}_r \Vr^T \Gprior^{-1}.
</script>
</p>
<p>Here, <script type="math/tex">\Vr \in \mathbb{R}^{n\times r}</script> contains only the <script type="math/tex">r</script>
generalized eigenvectors of <script type="math/tex">\matHmis</script> that correspond to the <script type="math/tex">r</script> largest eigenvalues,
which are assembled into the diagonal matrix <script type="math/tex">{\matrix{\Lambda}}_r = \diag
(\lambda_i) \in \mathbb{R}^{r \times r}</script>.</p>
<h5 id="the-approximate-posterior-covariance">The approximate posterior covariance</h5>
<p>Using the Sherman–Morrison–Woodbury formula, we write</p>
<p>
<script type="math/tex; mode=display">
\begin{align}
  \notag \Gpost = \left(\matHmis+ \Gprior^{-1}\right)^{-1}
  = \Gprior^{-1}-\Vr {\matrix{D}}_r \Vr^T +
  \mathcal{O}\left(\sum_{i=r+1}^{n} \frac{\lambda_i}{\lambda_i +
    1}\right),
\end{align}
</script>
</p>
<p>where <script type="math/tex">{\matrix{D}}_r :=\diag(\lambda_i/(\lambda_i+1)) \in
\mathbb{R}^{r\times r}</script>. The last term in this expression captures the
error due to truncation in terms of the discarded eigenvalues; this
provides a criterion for truncating the spectrum, namely that <script type="math/tex">r</script> is
chosen such that <script type="math/tex">\lambda_r</script> is small relative to 1. </p>
<p>Therefore we can approximate the posterior covariance as</p>
<p>
<script type="math/tex; mode=display">
\Gpost \approx \Gprior - \Vr {\matrix{D}}_r \Vr^T
</script>
</p>
<h5 id="drawing-samples-from-a-gaussian-distribution-with-covariance-gpost">Drawing samples from a Gaussian distribution with covariance <script type="math/tex">\Gpost</script>
</h5>
<p>Let <script type="math/tex">{\bf x}</script> be a sample for the prior distribution, i.e. <script type="math/tex">{\bf x} \sim \mathcal{N}({\bf 0}, \Gprior)</script>, then, using the low rank approximation of the posterior covariance, we compute a sample <script type="math/tex">{\bf v} \sim \mathcal{N}({\bf 0}, \Gpost)</script> as</p>
<p>
<script type="math/tex; mode=display">
  {\bf v} = \big\{ \Vr \big[ ({\matrix{\Lambda}}_r +
    \Ir)^{-1/2} - \Ir \big] \Vr^T\Gprior^{-1}  + {\bf I} \big\} {\bf x} 
</script>
</p>
<h4 id="full-posterior-sampling-via-markov-chain-monte-carlo-mcmc">Full posterior sampling via Markov chain Monte Carlo (MCMC)</h4>
<p>The posterior can be fully explored by using MCMC algorithms, the most popular method for sampling from a probability distribution.
In this example, some of the advanced MCMC algorithms are considered and compared in terms of efficiency and accuracy.</p>
<h5 id="the-preconditioned-crank-nicolson-algorithm-pcn">The preconditioned Crank-Nicolson algorithm (pCN)</h5>
<p>The pCN algorithm is perhaps the simplest MCMC method that is well-defined in the infinite
dimensional setting ensuring a mixing rates independent of the dimension of the discretized parameter space.</p>
<p>The algorithm proceeds as follows (see <a href="#Cotter">[Cotter et al. (2013)]</a> <a href="#Pinski">[Pinski et al. (2015)]</a> for the details):
1. Given <script type="math/tex">m^{(k)}</script>, propose <script type="math/tex">v^{(k+1)} = m_{\rm prop} + \sqrt{1 - \beta^2}(m^{(k)} - m_{\rm prop}) + \beta \xi^{(k)}, \quad \xi^{(k)} \sim \mathcal{N}( 0, \mathcal{C}_{\rm prop} )</script>
2. Set <script type="math/tex">m^{(k+1)} = v^{(k+1)}</script> with probability <script type="math/tex">a(m^{(k)}, v^{(k+1)}) = \min \left(1, 
\frac{\mu_{\text{post}}(v^{(k+1)}) q(v^{(k+1)}, m^{(k)})}{\mu_{\text{post}}(m^{(k)}) q(m^{(k)}, v^{(k+1)})} \right)</script>
</p>
<p>where <script type="math/tex">q(m,v) \sim \mathcal{N}\left( m_{\rm prop} + \sqrt{1 - \beta^2}(m - m_{\rm prop}), \beta^2 \mathcal{C}_{\rm prop} \right)</script> with proposal mean <script type="math/tex">m_{\rm prop}</script> and covariance <script type="math/tex">\mathcal{C}_{\rm prop}</script> and <script type="math/tex">\beta</script> is a parameter controlling the step length of the proposal.</p>
<h5 id="the-preconditioned-metropolis-adjusted-langevin-algorithm-mala">The preconditioned Metropolis adjusted Langevin algorithm (MALA)</h5>
<p>The MALA algorithm is built on two mechanisms: the overdamped Langevin diffusion to propose a move and the Metropolis–Hastings algorithm to accept or reject the proposal move <a href="#Roberts">[Roberts and Tweedie (1996)]</a>.</p>
<p>The preconditioned MALA algorithm is described as follows:
1. Given <script type="math/tex">m^{(k)}</script>, propose
<script type="math/tex">v^{(k+1)} = m^{(k)} + \tau \mathcal{A}_{\rm prop} \nabla \log \mu_{\text{post}} (m^{(k)}) + \sqrt{2 \tau \mathcal{A}_{\rm prop}} \xi^{(k)}, \quad \xi^{(k)} \sim \mathcal{N}( 0, \mathcal{I})</script>
2. Set <script type="math/tex">m^{(k+1)} = v^{(k+1)}</script> with probability <script type="math/tex">a(m^{(k)}, v^{(k+1)}) = \min \left(1, 
\frac{\mu_{\text{post}}(v^{(k+1)}) q(v^{(k+1)}, m^{(k)})}{\mu_{\text{post}}(m^{(k)}) q(m^{(k)}, v^{(k+1)})} \right)</script>
</p>
<p>where <script type="math/tex">q(m,v) \sim \mathcal{N}\left( m + \tau \mathcal{A}_{\rm prop} \nabla \log \mu_{\text{post}} (m), 2 \tau \mathcal{A}_{\rm prop} \right)</script> with a proposal covariance <script type="math/tex">\mathcal{A}_{\rm prop}</script> and <script type="math/tex">\tau</script> is a step size.</p>
<h5 id="the-delayed-rejection-dr">The Delayed Rejection (DR)</h5>
<p>The basic idea of the delayed rejection is to use a sequence of stages in each iteration.
Unlike the basic Metropolis-Hastings algorithm, if a candidate is rejected, a new move is proposed.
The acceptance rate for the new proposal move is adjusted so that the stationary distribution is preserved.
For the details, see <a href="#Mira">[Mira (2001)]</a>.</p>
<h3 id="this-tutorial-shows">This tutorial shows</h3>
<ul>
<li>Definition of the component of an inverse problem (the forward problem, the prior, and the misfit functional) using hIPPYlib</li>
<li>Computation of the maximum a posterior MAP point using inexact Newton-CG algorithm</li>
<li>Low-rank based approximation of the posterior covariance under the Laplace Approximation</li>
<li>Sampling from the prior distribution and Laplace Approximation using hIPPYlib</li>
<li>Construction of a MUQ workgraph using a PDE model defined in hIPPYlib</li>
<li>Exploring the full posterior using the MCMC methods implemented in MUQ</li>
<li>Convergence diagnostics of MCMC simulation results and their comparison</li>
</ul>
<h3 id="mathematical-tools-used">Mathematical tools used</h3>
<ul>
<li>Finite element method</li>
<li>Derivation of gradient and Hessian via the adjoint method</li>
<li>Inexact Newton-CG</li>
<li>Randomized eigensolvers</li>
<li>Bayes' formula</li>
<li>MCMC methods</li>
</ul>
<h3 id="list-of-software-used">List of software used</h3>
<p><a href="https://hippylib.github.io">hIPPYlib</a>, <a href="http://muq.mit.edu">MUQ</a> and their interfaces are the main software framework in this tutorial.
Additional tools used are:</p>
<ul>
<li><a href="http://fenicsproject.org">FEniCS</a>, A parallel finite element element library for the discretization of partial differential equations</li>
<li><a href="http://www.mcs.anl.gov/petsc">PETSc</a>, A set of data structures and routines for scalable and efficient linear algebra operations and solvers</li>
<li><a href="http://www.numpy.org">Numpy</a>, A python package for linear algebra</li>
<li><a href="http://matplotlib.org">Matplotlib</a>, A python package for visualizing the results</li>
</ul>
<h3 id="references">References</h3>
<p><a id="Cotter">Cotter, S. L., Roberts, G. O., Stuart, A. M., &amp; White, D. (2013)</a>. 
MCMC methods for functions: modifying old algorithms to make them faster. 
Statistical Science, 424-446.</p>
<p><a id="Pinski">Pinski, F. J., Simpson, G., Stuart, A. M., &amp; Weber, H. (2015)</a>. 
Algorithms for Kullback--Leibler approximation of probability measures in infinite dimensions. 
SIAM Journal on Scientific Computing, 37(6), A2733-A2757.</p>
<p><a id="Roberts">Roberts, G. O., &amp; Tweedie, R. L. (1996)</a>. 
Exponential convergence of Langevin distributions and their discrete approximations. 
Bernoulli, 2(4), 341-363.</p>
<p><a id="Mira">Mira, A. (2001)</a>.
On Metropolis-Hastings algorithms with delayed rejection.
Metron, 59(3-4), 231-241.</p>
<h2 id="ii-hippylib-muq-integration">II. hIPPYlib-MUQ integration</h2>
<p>The main objective of this example is to illustrate the interface between <a href="https://hippylib.github.io">hIPPYlib</a> and <a href="http://muq.mit.edu">MUQ</a>.</p>
<p>We make use of <a href="https://hippylib.github.io">hIPPYlib</a> to
- Define the forward model, prior distribution, and likelihood function
- Compute the MAP point by solving a deterministic inverse problem
- Construct the Laplace Approximation to the posterior distribution with a low-rank based approximation of the covariace operator.</p>
<p>The main classes and functions of <a href="https://hippylib.github.io">hIPPYlib</a> employed in this example are
- <code>hippylib::PDEVariationalProblem</code> : forward, adjoint and incremental problems solvers and their derivatives evaluations
- <code>hippylib::BiLaplacianPrior</code> : a biLaplacian Gaussian prior model
- <code>hippylib::GaussianLRPosterior</code> : the low rank Gaussian approximation of the posterior (used for generating starting points of MCMC simulations)</p>
<p><a href="http://muq.mit.edu">MUQ</a> is used to sample from the posterior by implementing MCMC methods with various kernels and proposals. </p>
<p>The main classes and functions used here are
- <code>pymuqModeling::PyModPiece</code> : an abstract interface for defining vector-valued models
- <code>pymuqModeling::PyGaussianBase</code> : an abstract interface for implementing Gaussian distributions
- <code>pymuqModeling::WorkGraph</code> : a graph or a frame of connected <code>pymuqModeling::PyModPiece</code> (or <code>pymuqModeling::WorkPiece</code>) classes
- <code>pymuqSamplingAlgorithms::CrankNicolsonProposal</code> : the pCN proposal
- <code>pymuqSamplingAlgorithms::MALAProposal</code> : the MALA proposal
- <code>pymuqSamplingAlgorithms::MHKernel</code> : the Metropolis-Hastings transition kernel
- <code>pymuqSamplingAlgorithms::DRKernel</code> : the delayed rejection kernel
- <code>pymuqSamplingAlgorithms::SingleChainMCMC</code> : a single chain MCMC sampler</p>
<p>To interface <a href="https://hippylib.github.io">hIPPYlib</a> and <a href="http://muq.mit.edu">MUQ</a> for this example, <code>hippymuq</code> provides the following classes:
- <code>hippymuq::Param2LogLikelihood</code> : a child of <code>muq::PyModPiece</code> which wraps <code>hippylib::PDEVariationalProblem</code> and <code>hippylib:PointwiseStateObservation</code> (solving the forward problem, mapping from parameters to log likelihood and evaluating its derivative)
- <code>hippymuq::BiLaplaceGaussian</code> : a child of <code>pymuqModeling::PyGaussianBase</code> which wraps <code>hippylib::BiLaplacianPrior</code>
- <code>hippymuq::LAPosteriorGaussian</code> : a child of <code>pymuqModeling::PyGaussianBase</code> which wraps <code>hippylib::GaussianLRPosterior</code></p>
<h2 id="iii-implementation">III. Implementation</h2>
<h3 id="1-load-modules">1. Load modules</h3>
<pre><code class="python">from __future__ import absolute_import, division, print_function

import math
import matplotlib.pyplot as plt
%matplotlib inline

import dolfin as dl
from hippylib import *
import pymuqModeling as mm
import pymuqSamplingAlgorithms as ms
import hippymuq as hm
import numpy as np

import logging
logging.getLogger('FFC').setLevel(logging.WARNING)
logging.getLogger('UFL').setLevel(logging.WARNING)
dl.set_log_active(False)

np.random.seed(seed=1)
</code></pre>

<h3 id="2-generate-the-true-parameter">2. Generate the true parameter</h3>
<p>This function generates a random field with a prescribed anisotropic covariance function.</p>
<pre><code class="python">def true_model(prior):
    noise = dl.Vector()
    prior.init_vector(noise, &quot;noise&quot;)
    parRandom.normal(1., noise)
    mtrue = dl.Vector()
    prior.init_vector(mtrue, 0)
    prior.sample(noise, mtrue)
    return mtrue
</code></pre>

<h3 id="3-set-up-the-mesh-and-finite-element-spaces">3. Set up the mesh and finite element spaces</h3>
<p>We compute a two dimensional mesh of a unit square with nx by ny elements.
We define a P2 finite element space for the <em>state</em> and <em>adjoint</em> variable and P1 for the <em>parameter</em>.</p>
<pre><code class="python">ndim = 2
nx   = 32
ny   = 32
mesh = dl.UnitSquareMesh(nx, ny)

Vh2 = dl.FunctionSpace(mesh, 'Lagrange', 2)
Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)
Vh  = [Vh2, Vh1, Vh2]
print(&quot;Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}&quot;.format(
    Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) )
</code></pre>

<pre><code>Number of dofs: STATE=4225, PARAMETER=1089, ADJOINT=4225
</code></pre>
<h3 id="4-set-up-the-forward-problem">4. Set up the forward problem</h3>
<p>Let <script type="math/tex">\Omega</script> be the unit square in <script type="math/tex">\mathbb{R}^2</script>, and <script type="math/tex">\Gamma_D</script>, <script type="math/tex">\Gamma_N</script>  be the Dirichlet and Neumann portitions of the boundary <script type="math/tex">\partial \Omega</script> (that is <script type="math/tex">\Gamma_D \cup \Gamma_N = \partial \Omega</script>, <script type="math/tex">\Gamma_D \cap \Gamma_N = \emptyset</script>). The forward problem reads</p>
<p>
<script type="math/tex; mode=display">
\left\{
\begin{array}{ll}
\nabla \cdot \left( e^m \nabla u\right) = f & \text{in } \Omega\\
u = u_D & \text{on } \Gamma_D, \\
e^m \nabla u \cdot \boldsymbol{n} = 0 & \text{on } \Gamma_N,
\end{array}
\right.
</script>
</p>
<p>where <script type="math/tex">u \in \mathcal{V}</script> is the state variable, and <script type="math/tex">m \in \mathcal{M}</script> is the uncertain parameter. Here <script type="math/tex">\Gamma_D</script> corresponds to the top and bottom sides of the unit square, and <script type="math/tex">\Gamma_N</script> corresponds to the left and right sides.
We also let <script type="math/tex">f = 0</script>, and <script type="math/tex">u_D = 1</script> on the top boundary and <script type="math/tex">u_D = 0</script> on the bottom boundary.</p>
<p>To set up the forward problem we use the <code>PDEVariationalProblem</code> class, which requires the following inputs
- the finite element spaces for the state, parameter, and adjoint variables <code>Vh</code>
- the pde in weak form <code>pde_varf</code>
- the boundary conditions <code>bc</code> for the forward problem and <code>bc0</code> for the adjoint and incremental problems.</p>
<p>The <code>PDEVariationalProblem</code> class offer the following functionality:
- solving the forward/adjoint and incremental problems
- evaluate first and second partial derivative of the forward problem with respect to the state, parameter, and adojnt variables.</p>
<pre><code class="python">def u_boundary(x, on_boundary):
    return on_boundary and ( x[1] &lt; dl.DOLFIN_EPS or x[1] &gt; 1.0 - dl.DOLFIN_EPS)

u_bdr  = dl.Expression(&quot;x[1]&quot;, degree=1)
u_bdr0 = dl.Constant(0.0)
bc  = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)
bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)

f = dl.Constant(0.0)

def pde_varf(u,m,p):
    return dl.exp(m)*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx - f*p*dl.dx

pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)
</code></pre>

<h3 id="5-set-up-the-prior">5. Set up the prior</h3>
<p>To obtain the synthetic true parameter <script type="math/tex">m_{\rm true}</script> we generate a realization from the prior distribution.</p>
<p>Here we assume a Gaussian prior, <script type="math/tex">\mu_{\rm prior} \sim \mathcal{N}(0, \prcov)</script> with zero mean and covariance matrix <script type="math/tex">\prcov = \mathcal{A}^{-2}</script>, which is implemented by <code>BiLaplacianPrior</code> class that provides methods to apply the regularization (precision) operator to a vector or to apply the prior covariance operator.</p>
<pre><code class="python">gamma = .1
delta = .5

theta0 = 2.
theta1 = .5
alpha  = math.pi/4

anis_diff = dl.CompiledExpression(ExpressionModule.AnisTensor2D(), degree = 1)
anis_diff.set(theta0, theta1, alpha)

prior = BiLaplacianPrior(Vh[PARAMETER], gamma, delta, anis_diff, robin_bc=True)
print(&quot;Prior regularization: (delta_x - gamma*Laplacian)^order: &quot;
      &quot;delta={0}, gamma={1}, order={2}&quot;.format(delta, gamma,2))

mtrue = true_model(prior)

objs = [dl.Function(Vh[PARAMETER],mtrue), dl.Function(Vh[PARAMETER],prior.mean)]
mytitles = [&quot;True Parameter&quot;, &quot;Prior mean&quot;]
nb.multi1_plot(objs, mytitles)
plt.show()
</code></pre>

<pre><code>Prior regularization: (delta_x - gamma*Laplacian)^order: delta=0.5, gamma=0.1, order=2
</code></pre>
<p><img alt="png" src="../SubsurfaceBayesian_files/SubsurfaceBayesian_11_1.png" /></p>
<h3 id="6-set-up-the-likelihood-and-generate-synthetic-observations">6. Set up the likelihood and generate synthetic observations</h3>
<p>To setup the observation operator <script type="math/tex">\mathcal{B}: \mathcal{V} \mapsto \mathbb{R}^{n_t}</script>, we generate <script type="math/tex">n_t</script> (<code>ntargets</code> in the code below) random locations where to evaluate the value of the state.</p>
<p>Under the assumption of Gaussian additive noise, the likelihood function <script type="math/tex">\pi_{\rm like}</script> has the form</p>
<p>
<script type="math/tex; mode=display">\pi_{\rm like}( \data \,| \, m ) \propto \exp\left( -\frac{1}{2}\|\mathcal{B}\,u(m) - \data \|^2_{\Gamma_{\rm noise}^{-1}}\right), </script>
</p>
<p>where <script type="math/tex">u(m)</script> denotes the solution of the forward model at a given parameter <script type="math/tex">m</script>.</p>
<p>The class <code>PointwiseStateObservation</code> implements the evaluation of the log-likelihood function and of its partial derivatives w.r.t. the state <script type="math/tex">u</script> and parameter <script type="math/tex">m</script>.</p>
<p>To generate the synthetic observation, we first solve the forward problem using the true parameter <script type="math/tex">m_{\rm true}</script>. Synthetic observations are obtained by perturbing the state variable at the observation points with a random Gaussian noise.
<code>rel_noise</code> is the signal to noise ratio.</p>
<pre><code class="python">ntargets  = 300
rel_noise = 0.005
targets   = np.random.uniform(0.05, 0.95, [ntargets, ndim])
print(&quot;Number of observation points: {0}&quot;.format(ntargets))

misfit = PointwiseStateObservation(Vh[STATE], targets)

utrue = pde.generate_state()
x = [utrue, mtrue, None]
pde.solveFwd(x[STATE], x)
misfit.B.mult(x[STATE], misfit.d)
MAX = misfit.d.norm(&quot;linf&quot;)
noise_std_dev = rel_noise * MAX
parRandom.normal_perturb(noise_std_dev, misfit.d)
misfit.noise_variance = noise_std_dev*noise_std_dev

model = Model(pde, prior, misfit)

vmax = max( utrue.max(), misfit.d.max() )
vmin = min( utrue.min(), misfit.d.min() )

plt.figure(figsize=(15,5))
nb.plot(dl.Function(Vh[STATE], utrue), mytitle=&quot;True State&quot;, 
        subplot_loc=121, vmin=vmin, vmax=vmax, cmap=&quot;jet&quot;)
nb.plot_pts(targets, misfit.d, mytitle=&quot;Observations&quot;, 
            subplot_loc=122, vmin=vmin, vmax=vmax, cmap=&quot;jet&quot;)
plt.show()
</code></pre>

<pre><code>Number of observation points: 300
</code></pre>
<p><img alt="png" src="../SubsurfaceBayesian_files/SubsurfaceBayesian_13_1.png" /></p>
<h3 id="7-compute-the-map-point">7. Compute the MAP point</h3>
<p>We used the globalized Newtown-CG method to compute the MAP point.</p>
<pre><code class="python">m = prior.mean.copy()
solver = ReducedSpaceNewtonCG(model)
solver.parameters[&quot;rel_tolerance&quot;]  = 1e-6
solver.parameters[&quot;abs_tolerance&quot;]  = 1e-12
solver.parameters[&quot;max_iter&quot;]       = 25
solver.parameters[&quot;GN_iter&quot;]        = 5
solver.parameters[&quot;globalization&quot;]  = &quot;LS&quot;
solver.parameters[&quot;LS&quot;][&quot;c_armijo&quot;] = 1e-4

x = solver.solve([None, m, None])

if solver.converged:
    print( &quot;\nConverged in &quot;, solver.it, &quot; iterations.&quot;)
else:
    print( &quot;\nNot Converged&quot;)

print( &quot;Termination reason:  &quot;, solver.termination_reasons[solver.reason] )
print( &quot;Final gradient norm: &quot;, solver.final_grad_norm )
print( &quot;Final cost:          &quot;, solver.final_cost )

plt.figure(figsize=(15,5))
nb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=121,mytitle=&quot;Recovered state&quot;, cmap=&quot;jet&quot;)
nb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=122,mytitle=&quot;MAP&quot;)
plt.show()

## true parameter state
</code></pre>

<pre><code>It  cg_it   cost       misfit     reg        (g,dm)     ||g||L2    alpha      tolcg     
  1     2   6.95e+03   6.94e+03   9.30e-01  -7.91e+04   1.78e+05   1.00e+00   5.00e-01
  2     3   2.81e+03   2.81e+03   1.36e+00  -8.27e+03   5.53e+04   1.00e+00   5.00e-01
  3     4   9.25e+02   9.21e+02   3.58e+00  -3.70e+03   2.53e+04   1.00e+00   3.77e-01
  4    10   3.39e+02   3.30e+02   9.32e+00  -1.30e+03   9.45e+03   1.00e+00   2.30e-01
  5     1   2.71e+02   2.61e+02   9.32e+00  -1.37e+02   1.41e+04   1.00e+00   2.81e-01
  6    13   1.73e+02   1.56e+02   1.63e+01  -1.96e+02   3.73e+03   1.00e+00   1.45e-01
  7    16   1.44e+02   1.20e+02   2.40e+01  -5.63e+01   1.78e+03   1.00e+00   1.00e-01
  8    12   1.41e+02   1.16e+02   2.45e+01  -6.92e+00   1.14e+03   1.00e+00   8.01e-02
  9    43   1.34e+02   9.87e+01   3.50e+01  -1.47e+01   8.67e+02   1.00e+00   6.98e-02
 10     3   1.34e+02   9.86e+01   3.50e+01  -1.31e-01   3.77e+02   1.00e+00   4.60e-02
 11    42   1.34e+02   9.85e+01   3.51e+01  -8.26e-02   8.90e+01   1.00e+00   2.24e-02
 12    59   1.34e+02   9.85e+01   3.51e+01  -7.70e-04   8.86e+00   1.00e+00   7.06e-03

Converged in  12  iterations.
Termination reason:   Norm of the gradient less than tolerance
Final gradient norm:  0.10248108935885225
Final cost:           133.60199155509807
</code></pre>
<p><img alt="png" src="../SubsurfaceBayesian_files/SubsurfaceBayesian_15_1.png" /></p>
<h3 id="8-compute-the-low-rank-based-laplace-approximation-of-the-posterior-la-posterior">8. Compute the low-rank based Laplace approximation of the posterior (LA-posterior)</h3>
<p>We used the <em>double pass</em> algorithm to compute a low-rank decomposition of the Hessian Misfit.
In particular, we solve</p>
<p>
<script type="math/tex; mode=display"> \matHmis {\bf v}_i = \lambda_i \Gprior^{-1} {\bf v}_i. </script>
</p>
<p>The effective rank of the Hessian misfit is the number of eigenvalues above the red line (<script type="math/tex">y=1</script>).
The effective rank is independent of the mesh size.</p>
<pre><code class="python">model.setPointForHessianEvaluations(x, gauss_newton_approx=False)
Hmisfit = ReducedHessian(model, misfit_only=True)
k = 100
p = 20
print( &quot;Single/Double Pass Algorithm. Requested eigenvectors: &quot;\
       &quot;{0}; Oversampling {1}.&quot;.format(k,p) )

Omega = MultiVector(x[PARAMETER], k+p)
parRandom.normal(1., Omega)
lmbda, V = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k)

nu = GaussianLRPosterior(prior, lmbda, V)
nu.mean = x[PARAMETER]

plt.plot(range(0,k), lmbda, 'b*', range(0,k+1), np.ones(k+1), '-r')
plt.yscale('log')
plt.xlabel('number')
plt.ylabel('eigenvalue')
plt.show()
</code></pre>

<pre><code>Single/Double Pass Algorithm. Requested eigenvectors: 100; Oversampling 20.
</code></pre>
<p><img alt="png" src="../SubsurfaceBayesian_files/SubsurfaceBayesian_17_1.png" /></p>
<h3 id="9-drawing-samples-from-the-prior-distribution-and-laplace-approximation">9. Drawing samples from the prior distribution and Laplace Approximation</h3>
<pre><code class="python">nsamples = 5
noise = dl.Vector()
nu.init_vector(noise,&quot;noise&quot;)
s_prior = dl.Function(Vh[PARAMETER], name=&quot;sample_prior&quot;)
s_post = dl.Function(Vh[PARAMETER], name=&quot;sample_post&quot;)

post_pw_variance, pr_pw_variance, corr_pw_variance =\
    nu.pointwise_variance(method=&quot;Exact&quot;)

pr_max   =  2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max()
pr_min   = -2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.min()
ps_max   =  2.5*math.sqrt( post_pw_variance.max() ) + nu.mean.max()
ps_min   = -2.5*math.sqrt( post_pw_variance.max() ) + nu.mean.min()

for i in range(nsamples):
    parRandom.normal(1., noise)
    nu.sample(noise, s_prior.vector(), s_post.vector())
    plt.figure(figsize=(15,5))
    nb.plot(s_prior, subplot_loc=121,mytitle=&quot;Prior sample&quot;, vmin=pr_min, vmax=pr_max)
    nb.plot(s_post, subplot_loc=122,mytitle=&quot;Laplace sample&quot;, vmin=ps_min, vmax=ps_max)
    plt.show()
</code></pre>

<p><img alt="png" src="../SubsurfaceBayesian_files/SubsurfaceBayesian_19_0.png" /></p>
<p><img alt="png" src="../SubsurfaceBayesian_files/SubsurfaceBayesian_19_1.png" /></p>
<p><img alt="png" src="../SubsurfaceBayesian_files/SubsurfaceBayesian_19_2.png" /></p>
<p><img alt="png" src="../SubsurfaceBayesian_files/SubsurfaceBayesian_19_3.png" /></p>
<p><img alt="png" src="../SubsurfaceBayesian_files/SubsurfaceBayesian_19_4.png" /></p>
<h3 id="10-define-a-quantify-of-interest">10 Define a quantify of interest</h3>
<p>As a quantity of interest, we consider the log of the flux through the bottom boundary:</p>
<p>
<script type="math/tex; mode=display"> q(m) = \ln \left\{ \int_{\Gamma_b} e^m \nabla u \cdot \mathbf{n} \, ds \right\}, </script>
</p>
<p>where the state variable <script type="math/tex">u</script> denotes the pressure, and <script type="math/tex">\mathbf{n}</script> is the unit normal vector to <script type="math/tex">\Gamma_b</script> (the bottom boundary of the domain).</p>
<pre><code class="python">class FluxQOI(object):
    def __init__(self, Vh, dsGamma):
        self.Vh = Vh
        self.dsGamma = dsGamma
        self.n = dl.Constant((0.,1.))

        self.u = None
        self.m = None
        self.L = {}

    def form(self, x):
        return dl.exp(x[PARAMETER])*dl.dot( dl.grad(x[STATE]), self.n)*self.dsGamma

    def eval(self, x):
        u = vector2Function(x[STATE], self.Vh[STATE])
        m = vector2Function(x[PARAMETER], self.Vh[PARAMETER])
        return np.log( dl.assemble(self.form([u,m])) )

class GammaBottom(dl.SubDomain):
    def inside(self, x, on_boundary):
        return ( abs(x[1]) &lt; dl.DOLFIN_EPS )

GC = GammaBottom()
marker = dl.MeshFunction(&quot;size_t&quot;, mesh, 1)
marker.set_all(0)
GC.mark(marker, 1)
dss = dl.Measure(&quot;ds&quot;, subdomain_data=marker)
qoi = FluxQOI(Vh,dss(1))
</code></pre>

<h3 id="11-exploring-the-posterior-using-mcmc-methods">11. Exploring the posterior using MCMC methods</h3>
<h4 id="define-the-parameter-to-observable-map-in-muq">Define the parameter-to-observable map in MUQ</h4>
<p>Overall, we want a mapping from parameter coefficients vector to the log target, <script type="math/tex">J(m) = - \tfrac{1}{2} \parallel {\bf f}(m) - \data \parallel^{2}_{{\bf \Gamma}_{\text{noise}}^{-1}} \! - \tfrac{1}{2}\parallel m - m_{\rm prior} \parallel^{2}_{\prcov^{-1}}</script>.
To do so, we generate a MUQ WorkGraph of connected ModPieces.</p>
<pre><code class="python"># a place holder ModPiece for the parameters
idparam = mm.IdentityOperator(Vh[PARAMETER].dim())

# log Gaussian Prior ModPiece
gaussprior = hm.BiLaplaceGaussian(prior)
log_gaussprior = gaussprior.AsDensity()

# parameter to log likelihood Modpiece
param2loglikelihood = hm.Param2LogLikelihood(pde, misfit)

# log target ModPiece
log_target = mm.DensityProduct(2)

workgraph = mm.WorkGraph()

# Identity operator for the parameters
workgraph.AddNode(idparam, 'Identity')

# Prior model
workgraph.AddNode(log_gaussprior, &quot;Log_prior&quot;)

# Likelihood model
workgraph.AddNode(param2loglikelihood, &quot;Log_likelihood&quot;)

# Posterior
workgraph.AddNode(log_target, &quot;Log_target&quot;)

workgraph.AddEdge(&quot;Identity&quot;, 0, &quot;Log_prior&quot;, 0)
workgraph.AddEdge(&quot;Log_prior&quot;, 0, &quot;Log_target&quot;, 0)

workgraph.AddEdge(&quot;Identity&quot;, 0, &quot;Log_likelihood&quot;, 0)
workgraph.AddEdge(&quot;Log_likelihood&quot;, 0, &quot;Log_target&quot;, 1)

workgraph.Visualize(&quot;workgraph.png&quot;)

# Construct the problem
problem = ms.SamplingProblem(workgraph.CreateModPiece(&quot;Log_target&quot;))
</code></pre>

<p><img alt="title" src="../workgraph.png" /></p>
<h4 id="set-up-mcmc-methods">Set up MCMC methods</h4>
<p>We run five different MCMC methods:
- <strong>pCN</strong>: Metropolis-Hastings kernel + pCN proposal with <script type="math/tex">m_{\rm prop} = m_{\rm prior} = 0</script> and <script type="math/tex">\mathcal{C}_{\rm prop} = \prcov</script>.
- <strong>h-pCN</strong>: Metropolis-Hastings kernel + pCN proposal with <script type="math/tex">m_{\rm prop} = \map</script> and <script type="math/tex">\mathcal{C}_{\rm prop} = \postcov</script>
- <strong>MALA</strong>: Metropolis-Hastings kernel + MALA proposal with <script type="math/tex">\mathcal{A}_{\rm prop} = \prcov</script>
- <strong>h-MALA</strong>: Metropolis-Hastings kernel + MALA proposal with <script type="math/tex">\mathcal{A}_{\rm prop} = \postcov</script>
- <strong>DR (h-pCN/h-MALA)</strong>: Delayed rejection kernel + two stage proposals (h-pCN proposal as first stage and h-MALA proposal as second stage)</p>
<p>where <script type="math/tex">\postcov</script> is the covariance of the LA-posterior.</p>
<p>We set the value of parameters (<script type="math/tex">\beta</script> for pCN and <script type="math/tex">\tau</script> for MALA) such that the acceptance rates are about 20-35% and 50-60% for pCN and MALA, respectively.</p>
<pre><code class="python"># Construct options for MH kernel and MCMC sampler
options = dict()
options['NumSamples'] = 22000  # Number of MCMC steps to take
options['BurnIn'] = 2000  # Number of steps to throw away as burn in
options['PrintLevel'] = 0  # in {0,1,2,3} Verbosity of the output

method_list = dict()

# pCN
opts = options.copy()
opts.update( {'Beta':0.005} )
gauss_pcn = hm.BiLaplaceGaussian(prior)
prop = ms.CrankNicolsonProposal(opts, problem, gauss_pcn)
kern = ms.MHKernel(opts, problem, prop)
sampler = ms.SingleChainMCMC(opts, [kern])

method_list['pCN'] = {'Options': opts, 'Sampler': sampler}

# h-pCN
opts = options.copy()
opts.update( {'Beta':0.55} )
gauss_hpcn = hm.LAPosteriorGaussian(nu)
prop = ms.CrankNicolsonProposal(opts, problem, gauss_hpcn)
kern = ms.MHKernel(opts, problem, prop)
sampler = ms.SingleChainMCMC(opts, [kern])

method_list['h-pCN'] = {'Options': opts, 'Sampler': sampler}

# MALA
opts = options.copy()
opts.update( {'StepSize':0.000006} )
gauss_mala = hm.BiLaplaceGaussian(prior, use_zero_mean=True)
prop = ms.MALAProposal(opts, problem, gauss_mala)
kern = ms.MHKernel(opts, problem, prop)
sampler = ms.SingleChainMCMC(opts, [kern])

method_list['MALA'] = {'Options': opts, 'Sampler': sampler}

# h-MALA
opts = options.copy()
opts.update( {'StepSize':0.1} )
gauss_hmala = hm.LAPosteriorGaussian(nu, use_zero_mean=True)
prop = ms.MALAProposal(opts, problem, gauss_hmala)
kern = ms.MHKernel(opts, problem, prop)
sampler = ms.SingleChainMCMC(opts, [kern])

method_list['h-MALA'] = {'Options': opts, 'Sampler': sampler}

# DR (h-pCN/h-MALA)
opts = options.copy()
opts.update( {'Beta':1.0, 'StepSize':0.1} )
gauss_dr1 = hm.LAPosteriorGaussian(nu)
gauss_dr2 = hm.LAPosteriorGaussian(nu, use_zero_mean=True)
prop1 = ms.CrankNicolsonProposal(opts, problem, gauss_dr1)
prop2 = ms.MALAProposal(opts, problem, gauss_dr2)
kern = ms.DRKernel( opts, problem, [prop1, prop2], [1.0, 1.0] )
sampler = ms.SingleChainMCMC(opts, [kern])

method_list['DR (h-pCN/h-MALA)'] = {'Options': opts, 'Sampler': sampler}

hm.print_methodDict(method_list)
</code></pre>

<pre><code>Method             Kernel     Proposal   Beta or Step-size
----------------------------------------------------------
pCN                mh         pcn           5.0e-03
h-pCN              mh         pcn           5.5e-01
MALA               mh         mala          6.0e-06
h-MALA             mh         mala          1.0e-01
DR (h-pCN/h-MALA)  dr         pcn           1.0e+00
                              mala          1.0e-01
</code></pre>
<h4 id="run-mcmc-methods">Run MCMC methods</h4>
<pre><code class="python"># Generate starting sample vector for all the MCMC simulations
noise = dl.Vector()
nu.init_vector(noise, &quot;noise&quot;)
parRandom.normal(1., noise)
pr_s = model.generate_vector(PARAMETER)
post_s = model.generate_vector(PARAMETER)
nu.sample(noise, pr_s, post_s, add_mean=True)
x0 = hm.dfVector2npArray(post_s)

# Implement MCMC simulations
for mName, method in method_list.items():
    # Run the MCMC sampler
    sampler = method['Sampler']
    samps = sampler.Run([x0])

    # Save the computed results
    method['Samples'] = samps
    method['ElapsedTime'] = sampler.TotalTime()

    kernel = sampler.Kernels()[0]
    if &quot;AcceptanceRate&quot; in dir(kernel):
        method['AcceptRate'] = kernel.AcceptanceRate()
    elif &quot;AcceptanceRates&quot; in dir(kernel):
        method['AcceptRate'] = kernel.AcceptanceRates()

    print(&quot;Drawn &quot;, options['NumSamples'] - options['BurnIn'] + 1, 
          &quot;MCMC samples using&quot;, mName)

print(&quot;\n&quot;)
print(&quot;Parameter space dimension:&quot;, Vh[PARAMETER].dim())
print(&quot;Number of samples:&quot;, options['NumSamples'] - options['BurnIn'] + 1)

# Keep track of the quantity of interest
qoi_dataset = hm.track_qoiTracer(pde, qoi, method_list)
hm.print_qoiResult(method_list, qoi_dataset)
hm.plot_qoiResult(method_list, qoi_dataset, max_lag=300)
</code></pre>

<p>Copyright &copy; 2020, Army Corps of Engineers, Massachusetts Institute of Technology, University of California--Merced, The University of Texas at Austin, Washington University in St. Louis<br>
All Rights reserved.<br></p>
<p><em>Acknowledgment</em>: This work is supported by the National Science Foundation under grants ACI-1550487, ACI-1550547, and ACI-1550593.</p></div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>This material is based on work partially supported by the National Science Foundation under Grants No ACI-1550487, ACI-1550547, ACI-1550593.</p>
            <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>var base_url = '..';</script>
        <script src="../js/base.js"></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script src="../search/require.js"></script>
        <script src="../search/search.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td><kbd>&larr;</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td><kbd>&rarr;</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>


    </body>
</html>
